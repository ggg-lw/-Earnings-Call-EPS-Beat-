{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3. 文本入库与特征工程\n",
        "\n",
        "本 notebook 目标：\n",
        "- 从 `data/transcripts` 读取 earnings call 文本\n",
        "- 解析 `Ticker / Fiscal Quarter / Call Datetime(ET)`\n",
        "- 按 `Prepared Remarks` / `Q&A` 分段，写入 SQLite\n",
        "- 基于 segment 文本计算文本特征（长度、pronoun、adverb、textstat、LM 字典）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "88b36d30",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PROJECT_ROOT: /Users/xinyuewang/Desktop/1.27\n",
            "TRANSCRIPTS_DIR: /Users/xinyuewang/Desktop/1.27/data/transcripts\n",
            "DB_PATH: /Users/xinyuewang/Desktop/1.27/data/earnings_calls.db\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "import sqlite3\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from textstat import (\n",
        "    automated_readability_index,\n",
        "    coleman_liau_index,\n",
        "    dale_chall_readability_score,\n",
        "    flesch_reading_ease,\n",
        "    flesch_kincaid_grade,\n",
        "    gunning_fog,\n",
        "    smog_index,\n",
        ")\n",
        "\n",
        "PROJECT_ROOT = Path(\"..\").resolve()\n",
        "TRANSCRIPTS_DIR = PROJECT_ROOT / \"data\" / \"transcripts\"\n",
        "DB_PATH = PROJECT_ROOT / \"data\" / \"earnings_calls.db\"\n",
        "\n",
        "print(\"PROJECT_ROOT:\", PROJECT_ROOT)\n",
        "print(\"TRANSCRIPTS_DIR:\", TRANSCRIPTS_DIR)\n",
        "print(\"DB_PATH:\", DB_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5f049c3",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "检查每个 ticker 的 transcript 文件名与财季完整性\n",
            "根目录: /Users/xinyuewang/Desktop/1.27/data/transcripts\n",
            "============================================================\n",
            "\n",
            "------------------------------------------------------------\n",
            "Ticker: BAC\n",
            "总文件数: 48\n",
            "保留文件数: 48\n",
            "财季范围: 2013-Q2 -> 2025-Q4\n",
            "中间缺失财季 3 个: ['2013-Q3', '2013-Q4', '2014-Q1']\n",
            "\n",
            "------------------------------------------------------------\n",
            "Ticker: BK\n",
            "总文件数: 43\n",
            "保留文件数: 43\n",
            "财季范围: 2015-Q1 -> 2025-Q4\n",
            "中间缺失财季 1 个: ['2021-Q3']\n",
            "\n",
            "------------------------------------------------------------\n",
            "Ticker: BLK\n",
            "总文件数: 44\n",
            "保留文件数: 44\n",
            "财季范围: 2015-Q1 -> 2025-Q4\n",
            "中间没有缺失的财季（按文件名解析）。\n",
            "\n",
            "------------------------------------------------------------\n",
            "Ticker: BR\n",
            "总文件数: 44\n",
            "保留文件数: 44\n",
            "财季范围: 2015-Q1 -> 2026-Q1\n",
            "中间缺失财季 1 个: ['2022-Q1']\n",
            "\n",
            "------------------------------------------------------------\n",
            "Ticker: C\n",
            "总文件数: 45\n",
            "保留文件数: 45\n",
            "财季范围: 2015-Q1 -> 2025-Q4\n",
            "中间没有缺失的财季（按文件名解析）。\n",
            "\n",
            "------------------------------------------------------------\n",
            "Ticker: DAL\n",
            "总文件数: 40\n",
            "保留文件数: 40\n",
            "财季范围: 2015-Q3 -> 2025-Q3\n",
            "中间缺失财季 1 个: ['2016-Q1']\n",
            "\n",
            "------------------------------------------------------------\n",
            "Ticker: GS\n",
            "总文件数: 44\n",
            "保留文件数: 44\n",
            "财季范围: 2015-Q1 -> 2025-Q4\n",
            "中间没有缺失的财季（按文件名解析）。\n",
            "\n",
            "------------------------------------------------------------\n",
            "Ticker: JPM\n",
            "总文件数: 44\n",
            "保留文件数: 44\n",
            "财季范围: 2015-Q1 -> 2025-Q4\n",
            "中间没有缺失的财季（按文件名解析）。\n",
            "\n",
            "------------------------------------------------------------\n",
            "Ticker: MS\n",
            "总文件数: 45\n",
            "保留文件数: 45\n",
            "财季范围: 2015-Q1 -> 2025-Q4\n",
            "中间没有缺失的财季（按文件名解析）。\n",
            "\n",
            "------------------------------------------------------------\n",
            "Ticker: MTB\n",
            "总文件数: 44\n",
            "保留文件数: 44\n",
            "财季范围: 2015-Q1 -> 2025-Q4\n",
            "中间没有缺失的财季（按文件名解析）。\n",
            "\n",
            "------------------------------------------------------------\n",
            "Ticker: PNC\n",
            "总文件数: 43\n",
            "保留文件数: 43\n",
            "财季范围: 2015-Q1 -> 2025-Q4\n",
            "中间缺失财季 1 个: ['2015-Q2']\n",
            "\n",
            "------------------------------------------------------------\n",
            "Ticker: STT\n",
            "总文件数: 43\n",
            "保留文件数: 43\n",
            "财季范围: 2015-Q1 -> 2025-Q4\n",
            "中间缺失财季 1 个: ['2017-Q4']\n",
            "\n",
            "------------------------------------------------------------\n",
            "Ticker: TSM\n",
            "总文件数: 41\n",
            "保留文件数: 41\n",
            "财季范围: 2015-Q1 -> 2025-Q4\n",
            "中间缺失财季 3 个: ['2017-Q4', '2024-Q3', '2024-Q4']\n",
            "\n",
            "------------------------------------------------------------\n",
            "Ticker: WFC\n",
            "总文件数: 44\n",
            "保留文件数: 44\n",
            "财季范围: 2014-Q2 -> 2025-Q3\n",
            "中间缺失财季 2 个: ['2016-Q4', '2017-Q1']\n"
          ]
        }
      ],
      "source": [
        "# 0. 检查每个 ticker 的 transcript 文件名与财季完整性\n",
        "\n",
        "from typing import List\n",
        "\n",
        "\n",
        "def _fq_to_index(fq: str) -> int:\n",
        "    \"\"\"把 'YYYY-Qn' 映射成可排序的整数: year*4 + (q-1)\"\"\"\n",
        "    try:\n",
        "        year_str, q_str = fq.split(\"-Q\")\n",
        "        return int(year_str) * 4 + int(q_str) - 1\n",
        "    except Exception:\n",
        "        return -1\n",
        "\n",
        "\n",
        "def inspect_transcript_files(base_dir: Path = TRANSCRIPTS_DIR):\n",
        "    \"\"\"检查每个 ticker 子目录下的文件名：\n",
        "    - 只保留文件名中包含 \"(TICKER)\" 的文件，其他文件删除\n",
        "    - 从文件名中解析财季，例如 \"Q1 2024\" -> \"2024-Q1\"\n",
        "    - 打印每个 ticker 的财季起止范围和中间是否有缺失的 earnings call\n",
        "    \"\"\"\n",
        "    print(\"=\" * 60)\n",
        "    print(\"检查每个 ticker 的 transcript 文件名与财季完整性\")\n",
        "    print(\"根目录:\", base_dir)\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    if not base_dir.exists():\n",
        "        print(f\"目录不存在: {base_dir}\")\n",
        "        return\n",
        "\n",
        "    ticker_dirs: List[Path] = sorted([p for p in base_dir.iterdir() if p.is_dir()])\n",
        "    if not ticker_dirs:\n",
        "        print(\"该目录下没有任何 ticker 子目录。\")\n",
        "        return\n",
        "\n",
        "    for ticker_dir in ticker_dirs:\n",
        "        ticker = ticker_dir.name.upper()\n",
        "        print(\"\\n\" + \"-\" * 60)\n",
        "        print(f\"Ticker: {ticker}\")\n",
        "        txt_files = sorted(ticker_dir.glob(\"*.txt\"))\n",
        "        print(f\"总文件数: {len(txt_files)}\")\n",
        "\n",
        "        valid_files = []\n",
        "        removed_files = []\n",
        "        fq_list = []  # 解析出的财季列表，比如 ['2019-Q1', ...]\n",
        "\n",
        "        ticker_tag = f\"({ticker})\"\n",
        "\n",
        "        for fp in txt_files:\n",
        "            name = fp.name\n",
        "\n",
        "            # 1) 剔除不属于当前 ticker 的文件（例如 MS 目录中的 EV 电话）\n",
        "            if ticker_tag not in name:\n",
        "                removed_files.append(name)\n",
        "                try:\n",
        "                    fp.unlink()\n",
        "                    print(f\"删除非 {ticker} 文件: {name}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"无法删除 {name}: {e}\")\n",
        "                continue\n",
        "\n",
        "            valid_files.append(name)\n",
        "\n",
        "            # 2) 从文件名中解析财季，形如 \"Q1 2024\" 或 \"Q3 2019\"\n",
        "            m = re.search(r\"Q([1-4])\\s+(\\d{4})\", name)\n",
        "            if m:\n",
        "                q = int(m.group(1))\n",
        "                year = int(m.group(2))\n",
        "                fq_list.append(f\"{year}-Q{q}\")\n",
        "\n",
        "        print(f\"保留文件数: {len(valid_files)}\")\n",
        "        if removed_files:\n",
        "            print(f\"剔除 {len(removed_files)} 个非 {ticker} 文件。\")\n",
        "\n",
        "        if not fq_list:\n",
        "            print(\"无法从文件名中解析出任何财季信息。\")\n",
        "            continue\n",
        "\n",
        "        # 去重并按时间排序\n",
        "        fq_unique = sorted(set(fq_list), key=_fq_to_index)\n",
        "        first_fq, last_fq = fq_unique[0], fq_unique[-1]\n",
        "        print(f\"财季范围: {first_fq} -> {last_fq}\")\n",
        "\n",
        "        # 构造完整的季度序列，检查中间是否缺失\n",
        "        start_year, start_q = map(int, first_fq.split(\"-Q\"))\n",
        "        end_year, end_q = map(int, last_fq.split(\"-Q\"))\n",
        "\n",
        "        full_range = []\n",
        "        y, q = start_year, start_q\n",
        "        while (y < end_year) or (y == end_year and q <= end_q):\n",
        "            full_range.append(f\"{y}-Q{q}\")\n",
        "            q += 1\n",
        "            if q == 5:\n",
        "                q = 1\n",
        "                y += 1\n",
        "\n",
        "        missing = [fq for fq in full_range if fq not in fq_unique]\n",
        "        if missing:\n",
        "            print(f\"中间缺失财季 {len(missing)} 个: {missing}\")\n",
        "        else:\n",
        "            print(\"中间没有缺失的财季（按文件名解析）。\")\n",
        "\n",
        "\n",
        "# 运行检查并收集缺失信息\n",
        "inspect_transcript_files()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "361fca12",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. 元数据解析 & 分段函数\n",
        "\n",
        "# 从文本前几行提取 Fiscal Quarter（例如 \"Q1 2024\"）\n",
        "FISCAL_QTR_PATTERN = re.compile(\n",
        "    r\"Q(?P<qtr>[1-4])\\s+(?P<year>\\d{4})\",\n",
        "    re.IGNORECASE,\n",
        ")\n",
        "\n",
        "# 从文本前几行提取 Call Date（例如 \"April 16, 2024 8:30 AM ET\" 或 \"January 15, 2025, 08:30 AM ET\"）\n",
        "# 支持年份后面有逗号的情况\n",
        "DATE_LINE_PATTERN = re.compile(\n",
        "    r\"(?P<month>[A-Za-z]+\\.?)\\s+(?P<day>\\d{1,2}),\\s+(?P<year>\\d{4}),?\\s+\"\n",
        "    r\"(?P<hour>\\d{1,2}):(?P<minute>\\d{2})\\s*(?P<ampm>AM|PM)\\s*ET\",\n",
        "    re.IGNORECASE,\n",
        ")\n",
        "\n",
        "MONTH_MAP = {\n",
        "    \"jan\": 1, \"january\": 1, \"jan.\": 1,\n",
        "    \"feb\": 2, \"february\": 2, \"feb.\": 2,\n",
        "    \"mar\": 3, \"march\": 3, \"mar.\": 3,\n",
        "    \"apr\": 4, \"april\": 4, \"apr.\": 4,\n",
        "    \"may\": 5,\n",
        "    \"jun\": 6, \"june\": 6, \"jun.\": 6,\n",
        "    \"jul\": 7, \"july\": 7, \"jul.\": 7,\n",
        "    \"aug\": 8, \"august\": 8, \"aug.\": 8,\n",
        "    \"sep\": 9, \"september\": 9, \"sept\": 9, \"sep.\": 9,\n",
        "    \"oct\": 10, \"october\": 10, \"oct.\": 10,\n",
        "    \"nov\": 11, \"november\": 11, \"nov.\": 11,\n",
        "    \"dec\": 12, \"december\": 12, \"dec.\": 12,\n",
        "}\n",
        "\n",
        "def parse_fiscal_quarter_from_text(text: str):\n",
        "    \"\"\"从文本前15行中提取 Fiscal Quarter（例如 2024-Q1）\n",
        "    跳过包含 'Call Start' 的行，只选择年份在合理范围内的（2000-2030）\n",
        "    \"\"\"\n",
        "    lines = text.split('\\n')[:15]\n",
        "    \n",
        "    # 过滤掉包含 \"Call Start\" 的行\n",
        "    filtered_lines = [line for line in lines if \"Call Start\" not in line]\n",
        "    search_text = '\\n'.join(filtered_lines)\n",
        "    \n",
        "    # 找到所有匹配的季度\n",
        "    matches = list(FISCAL_QTR_PATTERN.finditer(search_text))\n",
        "    if not matches:\n",
        "        return None\n",
        "    \n",
        "    # 选择年份在合理范围内的匹配\n",
        "    valid_matches = []\n",
        "    for m in matches:\n",
        "        year = int(m.group(\"year\"))\n",
        "        if 2000 <= year <= 2030:\n",
        "            valid_matches.append((m, year))\n",
        "    \n",
        "    if not valid_matches:\n",
        "        return None\n",
        "    \n",
        "    # 选择年份最大的匹配\n",
        "    best_match, _ = max(valid_matches, key=lambda x: x[1])\n",
        "    m = best_match\n",
        "    \n",
        "    year = int(m.group(\"year\"))\n",
        "    qtr = int(m.group(\"qtr\"))\n",
        "    return f\"{year}-Q{qtr}\"\n",
        "\n",
        "\n",
        "def parse_call_datetime_et(text: str):\n",
        "    \"\"\"从文本前15行中提取 Call Date（例如 2024-04-16 08:30）\n",
        "    跳过包含 'Call Start' 的行，只选择年份在合理范围内的日期（2000-2030）\n",
        "    \"\"\"\n",
        "    lines = text.split('\\n')[:15]\n",
        "    \n",
        "    # 过滤掉包含 \"Call Start\" 的行（这些行通常有错误的日期）\n",
        "    filtered_lines = [line for line in lines if \"Call Start\" not in line]\n",
        "    search_text = '\\n'.join(filtered_lines)\n",
        "    \n",
        "    # 找到所有匹配的日期\n",
        "    matches = list(DATE_LINE_PATTERN.finditer(search_text))\n",
        "    if not matches:\n",
        "        return None\n",
        "    \n",
        "    # 选择年份在合理范围内的匹配（2000-2030）\n",
        "    valid_matches = []\n",
        "    for m in matches:\n",
        "        year = int(m.group(\"year\"))\n",
        "        if 2000 <= year <= 2030:\n",
        "            valid_matches.append((m, year))\n",
        "    \n",
        "    if not valid_matches:\n",
        "        return None\n",
        "    \n",
        "    # 选择年份最大的匹配（通常是最新的正确日期）\n",
        "    best_match, _ = max(valid_matches, key=lambda x: x[1])\n",
        "    m = best_match\n",
        "\n",
        "    month_str = m.group(\"month\").lower()\n",
        "    month = MONTH_MAP.get(month_str)\n",
        "    if month is None:\n",
        "        return None\n",
        "\n",
        "    day = int(m.group(\"day\"))\n",
        "    year = int(m.group(\"year\"))\n",
        "    hour = int(m.group(\"hour\"))\n",
        "    minute = int(m.group(\"minute\"))\n",
        "    ampm = m.group(\"ampm\").upper()\n",
        "\n",
        "    if ampm == \"PM\" and hour != 12:\n",
        "        hour += 12\n",
        "    if ampm == \"AM\" and hour == 12:\n",
        "        hour = 0\n",
        "\n",
        "    try:\n",
        "        dt = datetime(year, month, day, hour, minute)\n",
        "        return dt.strftime(\"%Y-%m-%d %H:%M\")\n",
        "    except ValueError as e:\n",
        "        # 如果日期无效（例如 2月30日），返回 None\n",
        "        return None\n",
        "\n",
        "\n",
        "# Q&A 分割标记（多种变体）\n",
        "QA_MARKERS = [\n",
        "    r\"Question-and-Answer Session\",\n",
        "    r\"Questions and Answers\",\n",
        "    r\"Question and Answer Session\",\n",
        "    r\"Q&A Session\",\n",
        "    r\"Q & A Session\",\n",
        "]\n",
        "\n",
        "def find_qa_marker(text: str, skip_first_n_sentences: int = 10):\n",
        "    \"\"\"找到第一个 Q&A 标记的位置\n",
        "    从第 skip_first_n_sentences 句话之后开始搜索，避免在开头误匹配\n",
        "    \"\"\"\n",
        "    # 先找到前 N 句话的结束位置\n",
        "    sentence_endings = re.finditer(r'[.!?]+\\s+', text)\n",
        "    sentence_positions = [m.end() for m in sentence_endings]\n",
        "    \n",
        "    # 如果句子数少于 skip_first_n_sentences，从开头开始搜索\n",
        "    if len(sentence_positions) < skip_first_n_sentences:\n",
        "        search_start_pos = 0\n",
        "    else:\n",
        "        # 从第 skip_first_n_sentences 句话之后开始搜索\n",
        "        search_start_pos = sentence_positions[skip_first_n_sentences - 1]\n",
        "    \n",
        "    # 只搜索 search_start_pos 之后的内容\n",
        "    search_text = text[search_start_pos:]\n",
        "    \n",
        "    # 在搜索文本中查找 Q&A 标记\n",
        "    for marker in QA_MARKERS:\n",
        "        m = re.search(marker, search_text, flags=re.IGNORECASE)\n",
        "        if m:\n",
        "            # 返回在原文中的绝对位置\n",
        "            return search_start_pos + m.start()\n",
        "    \n",
        "    return None\n",
        "\n",
        "\n",
        "def split_transcript_into_segments(text: str):\n",
        "    \"\"\"\n",
        "    分段逻辑：\n",
        "    - 前半部分（到 Q&A 标记之前）自动是 Prepared Remarks\n",
        "    - 如果找到 Q&A 标记，之后的部分是 Q&A\n",
        "    - segment_type 只能是 \"Prepared Remarks\" 或 \"Q&A\"\n",
        "    \"\"\"\n",
        "    segments = []\n",
        "    \n",
        "    qa_start_pos = find_qa_marker(text)\n",
        "    \n",
        "    if qa_start_pos is not None:\n",
        "        # 找到 Q&A 标记，分成两部分\n",
        "        prepared_text = text[:qa_start_pos].strip()\n",
        "        qa_text = text[qa_start_pos:].strip()\n",
        "        \n",
        "        if prepared_text:\n",
        "            segments.append({\n",
        "                \"segment_type\": \"Prepared Remarks\",\n",
        "                \"text\": prepared_text\n",
        "            })\n",
        "        if qa_text:\n",
        "            segments.append({\n",
        "                \"segment_type\": \"Q&A\",\n",
        "                \"text\": qa_text\n",
        "            })\n",
        "    else:\n",
        "        # 没找到 Q&A 标记，整篇作为 Prepared Remarks\n",
        "        segments.append({\n",
        "            \"segment_type\": \"Prepared Remarks\",\n",
        "            \"text\": text.strip()\n",
        "        })\n",
        "    \n",
        "    return segments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "6b78bfb8",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DB 初始化完成\n"
          ]
        }
      ],
      "source": [
        "# 2. 初始化 SQLite 表\n",
        "\n",
        "def init_db(db_path: Path):\n",
        "    conn = sqlite3.connect(db_path)\n",
        "    cur = conn.cursor()\n",
        "\n",
        "    cur.execute(\n",
        "        \"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS segments (\n",
        "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "            ticker TEXT,\n",
        "            fiscal_quarter TEXT,\n",
        "            call_datetime_et TEXT,\n",
        "            segment_type TEXT,\n",
        "            text_content TEXT,\n",
        "            source_file TEXT\n",
        "        )\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "    cur.execute(\n",
        "        \"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS segment_features (\n",
        "            segment_id INTEGER PRIMARY KEY,\n",
        "            n_words INTEGER,\n",
        "            n_sentences INTEGER,\n",
        "            avg_words_per_sentence REAL,\n",
        "            pronoun_plural_ratio REAL,\n",
        "            adverb_ratio REAL,\n",
        "            ari REAL,\n",
        "            coleman_liau REAL,\n",
        "            dale_chall REAL,\n",
        "            flesch_ease REAL,\n",
        "            flesch_kincaid REAL,\n",
        "            gunning_fog REAL,\n",
        "            smog REAL,\n",
        "            lm_positive REAL,\n",
        "            lm_negative REAL,\n",
        "            lm_uncertainty REAL,\n",
        "            lm_litigous REAL,\n",
        "            lm_superfluous REAL,\n",
        "            lm_interesting REAL,\n",
        "            lm_modal_weak REAL,\n",
        "            lm_modal_moderate REAL,\n",
        "            lm_modal_strong REAL,\n",
        "            lm_constraining REAL,\n",
        "            lm_complexity REAL,\n",
        "            lm_net_sentiment REAL,\n",
        "            lm_polarity REAL,\n",
        "            lm_subjectivity REAL,\n",
        "            FOREIGN KEY(segment_id) REFERENCES segments(id)\n",
        "        )\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "\n",
        "init_db(DB_PATH)\n",
        "print(\"DB 初始化完成\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "5beba98e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "⚠️ 无法解析 Fiscal Quarter: Morgan Stanley. (MS) Management on Bank of America Future of Financials Virtual Conference 2020 - Earnings Call Transcript.txt\n",
            "⚠️ 无法解析 Fiscal Quarter: The PNC Financial Services Group, Inc. (PNC) Q2 2024 Earnings Call Transcript.txt\n",
            "共处理 transcript 文件数: 612\n",
            "写入 segment 记录数: 1218\n",
            "⚠️ 失败文件数: 2\n",
            "前5个失败文件: ['Morgan Stanley. (MS) Management on Bank of America Future of Financials Virtual Conference 2020 - Earnings Call Transcript.txt', 'The PNC Financial Services Group, Inc. (PNC) Q2 2024 Earnings Call Transcript.txt']\n"
          ]
        }
      ],
      "source": [
        "# 3. 将 transcripts 写入 segments 表\n",
        "\n",
        "def insert_segments_from_transcripts(db_path: Path, transcripts_dir: Path):\n",
        "    conn = sqlite3.connect(db_path)\n",
        "    cur = conn.cursor()\n",
        "\n",
        "    # 清空现有数据（如果需要重新导入）\n",
        "    cur.execute(\"DELETE FROM segments\")\n",
        "    cur.execute(\"DELETE FROM segment_features\")\n",
        "\n",
        "    count_files = 0\n",
        "    count_segments = 0\n",
        "    failed_files = []\n",
        "\n",
        "    for ticker_dir in sorted(transcripts_dir.glob(\"*\")):\n",
        "        if not ticker_dir.is_dir():\n",
        "            continue\n",
        "\n",
        "        # Ticker 直接从文件夹名提取（例如 \"BAC\", \"JPM\"）\n",
        "        ticker = ticker_dir.name.upper()\n",
        "\n",
        "        for txt_path in sorted(ticker_dir.glob(\"*.txt\")):\n",
        "            count_files += 1\n",
        "\n",
        "            try:\n",
        "                with open(txt_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                    text = f.read()\n",
        "\n",
        "                # 从文本前几行提取 Fiscal Quarter 和 Call Date\n",
        "                fiscal_quarter = parse_fiscal_quarter_from_text(text)\n",
        "                call_datetime_et = parse_call_datetime_et(text)\n",
        "\n",
        "                if not fiscal_quarter:\n",
        "                    print(f\"⚠️ 无法解析 Fiscal Quarter: {txt_path.name}\")\n",
        "                    failed_files.append(txt_path.name)\n",
        "                    continue\n",
        "\n",
        "                segments = split_transcript_into_segments(text)\n",
        "\n",
        "                for seg in segments:\n",
        "                    cur.execute(\n",
        "                        \"\"\"\n",
        "                        INSERT INTO segments (\n",
        "                            ticker, fiscal_quarter, call_datetime_et,\n",
        "                            segment_type, text_content, source_file\n",
        "                        ) VALUES (?, ?, ?, ?, ?, ?)\n",
        "                        \"\"\",\n",
        "                        (\n",
        "                            ticker,\n",
        "                            fiscal_quarter,\n",
        "                            call_datetime_et,\n",
        "                            seg[\"segment_type\"],\n",
        "                            seg[\"text\"],\n",
        "                            str(txt_path.relative_to(transcripts_dir.parent)),\n",
        "                        ),\n",
        "                    )\n",
        "                    count_segments += 1\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"❌ 处理文件失败 {txt_path.name}: {e}\")\n",
        "                failed_files.append(txt_path.name)\n",
        "\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "\n",
        "    print(f\"共处理 transcript 文件数: {count_files}\")\n",
        "    print(f\"写入 segment 记录数: {count_segments}\")\n",
        "    if failed_files:\n",
        "        print(f\"⚠️ 失败文件数: {len(failed_files)}\")\n",
        "        print(\"前5个失败文件:\", failed_files[:5])\n",
        "\n",
        "\n",
        "insert_segments_from_transcripts(DB_PATH, TRANSCRIPTS_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "7069e6df",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4. 基本文本特征（长度、pronoun、adverb）\n",
        "\n",
        "PRONOUNS_ALL = {\n",
        "    \"i\", \"me\", \"my\", \"mine\",\n",
        "    \"you\", \"your\", \"yours\",\n",
        "    \"he\", \"him\", \"his\",\n",
        "    \"she\", \"her\", \"hers\",\n",
        "    \"it\", \"its\",\n",
        "    \"we\", \"us\", \"our\", \"ours\",\n",
        "    \"they\", \"them\", \"their\", \"theirs\",\n",
        "}\n",
        "\n",
        "PRONOUNS_PLURAL = {\n",
        "    \"we\", \"us\", \"our\", \"ours\",\n",
        "    \"they\", \"them\", \"their\", \"theirs\",\n",
        "}\n",
        "\n",
        "COMMON_ADVERBS = {\n",
        "    \"very\", \"quite\", \"rather\", \"extremely\", \"highly\", \"significantly\",\n",
        "    \"substantially\", \"slightly\", \"barely\", \"rarely\", \"frequently\",\n",
        "    \"usually\", \"typically\", \"generally\", \"probably\", \"possibly\",\n",
        "    \"certainly\", \"clearly\", \"obviously\",\n",
        "}\n",
        "\n",
        "WORD_PATTERN = re.compile(r\"[A-Za-z']+\")\n",
        "\n",
        "def tokenize_words(text: str):\n",
        "    return WORD_PATTERN.findall(text.lower())\n",
        "\n",
        "\n",
        "def count_sentences(text: str):\n",
        "    parts = re.split(r\"[.!?]+\", text)\n",
        "    return sum(1 for p in parts if p.strip())\n",
        "\n",
        "\n",
        "def basic_features(text: str):\n",
        "    tokens = tokenize_words(text)\n",
        "    n_words = len(tokens)\n",
        "    n_sent = count_sentences(text)\n",
        "    avg_words = n_words / n_sent if n_sent > 0 else 0.0\n",
        "\n",
        "    pronouns = [w for w in tokens if w in PRONOUNS_ALL]\n",
        "    pronouns_plural = [w for w in pronouns if w in PRONOUNS_PLURAL]\n",
        "\n",
        "    pronoun_plural_ratio = (\n",
        "        len(pronouns_plural) / len(pronouns) if len(pronouns) > 0 else 0.0\n",
        "    )\n",
        "\n",
        "    adverbs = [\n",
        "        w for w in tokens\n",
        "        if w in COMMON_ADVERBS or (len(w) > 3 and w.endswith(\"ly\"))\n",
        "    ]\n",
        "    adverb_ratio = len(adverbs) / n_words if n_words > 0 else 0.0\n",
        "\n",
        "    return {\n",
        "        \"n_words\": n_words,\n",
        "        \"n_sentences\": n_sent,\n",
        "        \"avg_words_per_sentence\": avg_words,\n",
        "        \"pronoun_plural_ratio\": pronoun_plural_ratio,\n",
        "        \"adverb_ratio\": adverb_ratio,\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "ecd7827d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5. textstat 可读性特征\n",
        "\n",
        "def readability_features(text: str):\n",
        "    try:\n",
        "        ari = automated_readability_index(text)\n",
        "    except Exception:\n",
        "        ari = None\n",
        "    try:\n",
        "        cl = coleman_liau_index(text)\n",
        "    except Exception:\n",
        "        cl = None\n",
        "    try:\n",
        "        dc = dale_chall_readability_score(text)\n",
        "    except Exception:\n",
        "        dc = None\n",
        "    try:\n",
        "        fre = flesch_reading_ease(text)\n",
        "    except Exception:\n",
        "        fre = None\n",
        "    try:\n",
        "        fk = flesch_kincaid_grade(text)\n",
        "    except Exception:\n",
        "        fk = None\n",
        "    try:\n",
        "        gf = gunning_fog(text)\n",
        "    except Exception:\n",
        "        gf = None\n",
        "    try:\n",
        "        smg = smog_index(text)\n",
        "    except Exception:\n",
        "        smg = None\n",
        "\n",
        "    return {\n",
        "        \"ari\": ari,\n",
        "        \"coleman_liau\": cl,\n",
        "        \"dale_chall\": dc,\n",
        "        \"flesch_ease\": fre,\n",
        "        \"flesch_kincaid\": fk,\n",
        "        \"gunning_fog\": gf,\n",
        "        \"smog\": smg,\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "fcedfc51",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LM 词典加载完成\n"
          ]
        }
      ],
      "source": [
        "# 6. LM 字典特征（需要 data/LM/LM_MasterDictionary.csv）\n",
        "\n",
        "LM_CSV_PATH = PROJECT_ROOT / \"data\" / \"LM\" / \"LM_MasterDictionary.csv\"\n",
        "\n",
        "if LM_CSV_PATH.exists():\n",
        "    lm_df = pd.read_csv(LM_CSV_PATH)\n",
        "    lm_df[\"Word\"] = lm_df[\"Word\"].astype(str).str.lower()\n",
        "\n",
        "    def build_lm_set(col):\n",
        "        if col not in lm_df.columns:\n",
        "            return set()\n",
        "        return set(lm_df.loc[lm_df[col] > 0, \"Word\"])\n",
        "\n",
        "    LM_POSITIVE = build_lm_set(\"Positive\")\n",
        "    LM_NEGATIVE = build_lm_set(\"Negative\")\n",
        "    LM_UNCERTAINTY = build_lm_set(\"Uncertainty\")\n",
        "    LM_LITIGIOUS = build_lm_set(\"Litigious\")\n",
        "    LM_SUPERFLUOUS = build_lm_set(\"Superfluous\")\n",
        "    LM_INTERESTING = build_lm_set(\"Interesting\")\n",
        "    LM_MODAL_WEAK = build_lm_set(\"ModalWeak\")\n",
        "    LM_MODAL_MODERATE = build_lm_set(\"ModalModerate\")\n",
        "    LM_MODAL_STRONG = build_lm_set(\"ModalStrong\")\n",
        "    LM_CONSTRAINING = build_lm_set(\"Constraining\")\n",
        "    LM_COMPLEXITY = build_lm_set(\"Complexity\")\n",
        "\n",
        "    print(\"LM 词典加载完成\")\n",
        "else:\n",
        "    print(\"⚠️ 未找到 LM_MasterDictionary.csv，LM 特征全部为 0\")\n",
        "    LM_POSITIVE = LM_NEGATIVE = LM_UNCERTAINTY = LM_LITIGIOUS = set()\n",
        "    LM_SUPERFLUOUS = LM_INTERESTING = set()\n",
        "    LM_MODAL_WEAK = LM_MODAL_MODERATE = LM_MODAL_STRONG = set()\n",
        "    LM_CONSTRAINING = LM_COMPLEXITY = set()\n",
        "\n",
        "\n",
        "def lm_features(text: str):\n",
        "    tokens = tokenize_words(text)\n",
        "    n_words = len(tokens) if len(tokens) > 0 else 1\n",
        "\n",
        "    def ratio(lm_set):\n",
        "        return sum(1 for w in tokens if w in lm_set) / n_words\n",
        "\n",
        "    pos = ratio(LM_POSITIVE)\n",
        "    neg = ratio(LM_NEGATIVE)\n",
        "    unc = ratio(LM_UNCERTAINTY)\n",
        "    lit = ratio(LM_LITIGIOUS)\n",
        "    sup = ratio(LM_SUPERFLUOUS)\n",
        "    intr = ratio(LM_INTERESTING)\n",
        "    mweak = ratio(LM_MODAL_WEAK)\n",
        "    mmod = ratio(LM_MODAL_MODERATE)\n",
        "    mstrong = ratio(LM_MODAL_STRONG)\n",
        "    constr = ratio(LM_CONSTRAINING)\n",
        "    complx = ratio(LM_COMPLEXITY)\n",
        "\n",
        "    net_sentiment = pos - neg\n",
        "    polarity = pos + neg\n",
        "    subjectivity = pos + neg + unc + lit + sup + intr\n",
        "\n",
        "    return {\n",
        "        \"lm_positive\": pos,\n",
        "        \"lm_negative\": neg,\n",
        "        \"lm_uncertainty\": unc,\n",
        "        \"lm_litigous\": lit,\n",
        "        \"lm_superfluous\": sup,\n",
        "        \"lm_interesting\": intr,\n",
        "        \"lm_modal_weak\": mweak,\n",
        "        \"lm_modal_moderate\": mmod,\n",
        "        \"lm_modal_strong\": mstrong,\n",
        "        \"lm_constraining\": constr,\n",
        "        \"lm_complexity\": complx,\n",
        "        \"lm_net_sentiment\": net_sentiment,\n",
        "        \"lm_polarity\": polarity,\n",
        "        \"lm_subjectivity\": subjectivity,\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "6372edea",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "需要计算特征的 segment 数量: 1218\n",
            "已写入 200 / 1218\n",
            "已写入 400 / 1218\n",
            "已写入 600 / 1218\n",
            "已写入 800 / 1218\n",
            "已写入 1000 / 1218\n",
            "已写入 1200 / 1218\n",
            "全部特征写入完成\n"
          ]
        }
      ],
      "source": [
        "# 7. 计算并写入所有 segment 的特征\n",
        "\n",
        "def compute_and_store_features(db_path: Path, batch_size: int = 200):\n",
        "    conn = sqlite3.connect(db_path)\n",
        "    cur = conn.cursor()\n",
        "\n",
        "    cur.execute(\n",
        "        \"\"\"\n",
        "        SELECT s.id, s.text_content\n",
        "        FROM segments s\n",
        "        LEFT JOIN segment_features f ON s.id = f.segment_id\n",
        "        WHERE f.segment_id IS NULL\n",
        "        \"\"\"\n",
        "    )\n",
        "    rows = cur.fetchall()\n",
        "\n",
        "    print(\"需要计算特征的 segment 数量:\", len(rows))\n",
        "\n",
        "    to_insert = []\n",
        "    for idx, (seg_id, text) in enumerate(rows, 1):\n",
        "        text = text or \"\"\n",
        "        feat_basic = basic_features(text)\n",
        "        feat_read = readability_features(text)\n",
        "        feat_lm = lm_features(text)\n",
        "\n",
        "        record = {\n",
        "            \"segment_id\": seg_id,\n",
        "            **feat_basic,\n",
        "            **feat_read,\n",
        "            **feat_lm,\n",
        "        }\n",
        "        to_insert.append(record)\n",
        "\n",
        "        if len(to_insert) >= batch_size:\n",
        "            df = pd.DataFrame(to_insert)\n",
        "            df.to_sql(\"segment_features\", conn, if_exists=\"append\", index=False)\n",
        "            print(f\"已写入 {idx} / {len(rows)}\")\n",
        "            to_insert = []\n",
        "\n",
        "    if to_insert:\n",
        "        df = pd.DataFrame(to_insert)\n",
        "        df.to_sql(\"segment_features\", conn, if_exists=\"append\", index=False)\n",
        "\n",
        "    conn.close()\n",
        "    print(\"全部特征写入完成\")\n",
        "\n",
        "\n",
        "compute_and_store_features(DB_PATH)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "lm312",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
