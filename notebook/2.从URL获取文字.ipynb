{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3109b5ef",
      "metadata": {},
      "source": [
        "# 2. ä» URL è·å–æ–‡å­—\n",
        "\n",
        "ä¸‹è½½æ‰€æœ‰ Earnings Call Transcript æ–‡æœ¬ï¼Œæ”¯æŒï¼š\n",
        "- è‡ªåŠ¨æ£€æµ‹ç¼ºå¤±å¹´ä»½å¹¶é‡æ–°ä¸‹è½½\n",
        "- åªä¸‹è½½è¿‘10å¹´ï¼ˆ2015å¹´åŠä»¥åï¼‰çš„æ•°æ®\n",
        "- ä½¿ç”¨ Chrome 9222 ç«¯å£è¿æ¥\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "89f0c622",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "é¡¹ç›®æ ¹ç›®å½•: /Users/xinyuewang/Desktop/1.27\n",
            "JSONç´¢å¼•ç›®å½•: /Users/xinyuewang/Desktop/1.27/data/transcript_index\n",
            "Transcriptä¿å­˜ç›®å½•: /Users/xinyuewang/Desktop/1.27/data/transcripts\n",
            "ç›®æ ‡å…¬å¸æ•°: 28\n",
            "æ—¶é—´èŒƒå›´: 2015å¹´åŠä»¥å\n"
          ]
        }
      ],
      "source": [
        "# ========== é…ç½®å’Œå¯¼å…¥ ==========\n",
        "\n",
        "import os\n",
        "import time\n",
        "import glob\n",
        "import json\n",
        "import random\n",
        "import re\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from typing import Optional, Tuple\n",
        "\n",
        "import asyncio\n",
        "from playwright.async_api import async_playwright\n",
        "from playwright.async_api._generated import Page\n",
        "\n",
        "# 16 å®¶å…¬å¸åˆ—è¡¨ï¼ˆä¸ notebook 1 ä¿æŒä¸€è‡´ï¼‰\n",
        "TICKER_LIST = [\n",
        "    # Banks / Financial (14)\n",
        "    \"JPM\",  # JPMorgan Chase\n",
        "    \"BK\",   # Bank of New York Mellon\n",
        "    \"DAL\",  # Delta Airlines\n",
        "    \"C\",    # Citigroup\n",
        "    \"WFC\",  # Wells Fargo\n",
        "    \"BAC\",  # Bank of America\n",
        "    \"TSM\",  # TSMC\n",
        "    \"MS\",   # Morgan Stanley\n",
        "    \"GS\",   # Goldman Sachs\n",
        "    \"BLK\",  # BlackRock\n",
        "    \"MTB\",  # M&T Bank\n",
        "    \"STT\",  # State Street\n",
        "    \"PNC\",  # PNC Financial\n",
        "    \"BR\",   # Broadridge Financial\n",
        "\n",
        "    # Tech / Mega Caps (add 16 â†’ total 30)\n",
        "    \"AAPL\",  # Apple\n",
        "    \"MSFT\",  # Microsoft\n",
        "    \"AMZN\",  # Amazon\n",
        "    \"NVDA\",  # Nvidia\n",
        "    \"TSLA\",  # Tesla\n",
        "    \"NFLX\",  # Netflix\n",
        "\n",
        "    \"AMD\",   # Advanced Micro Devices\n",
        "    \"AVGO\",  # Broadcom\n",
        "    \"INTC\",  # Intel\n",
        "    \"QCOM\",  # Qualcomm\n",
        "\n",
        "    \"CRM\",   # Salesforce\n",
        "    \"ADBE\",  # Adobe\n",
        "    \"NOW\",   # ServiceNow\n",
        "    \"SHOP\",  # Shopify\n",
        "]\n",
        "\n",
        "# åªä¸‹è½½è¿‘10å¹´çš„æ•°æ®ï¼ˆ2015å¹´åŠä»¥åï¼‰\n",
        "MIN_YEAR = 2015\n",
        "\n",
        "PROJECT_ROOT = Path(\"..\").resolve()\n",
        "JSON_DIR = PROJECT_ROOT / \"data\" / \"transcript_index\"\n",
        "MAIN_OUTPUT_DIR = PROJECT_ROOT / \"data\" / \"transcripts\"\n",
        "\n",
        "print(f\"é¡¹ç›®æ ¹ç›®å½•: {PROJECT_ROOT}\")\n",
        "print(f\"JSONç´¢å¼•ç›®å½•: {JSON_DIR}\")\n",
        "print(f\"Transcriptä¿å­˜ç›®å½•: {MAIN_OUTPUT_DIR}\")\n",
        "print(f\"ç›®æ ‡å…¬å¸æ•°: {len(TICKER_LIST)}\")\n",
        "print(f\"æ—¶é—´èŒƒå›´: {MIN_YEAR}å¹´åŠä»¥å\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "03fdc9ef",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3. ä¸‹è½½å•ä¸ª URL çš„ transcriptï¼ˆç”¨äºè¡¥å…¨ç¼ºå¤±çš„å­£åº¦ï¼‰\n",
        "# æ³¨æ„ï¼šæ­¤ cell éœ€è¦åœ¨è¿è¡Œ Cell 4 å’Œ Cell 5ï¼ˆå‡½æ•°å®šä¹‰ï¼‰ä¹‹åæ‰èƒ½è¿è¡Œ\n",
        "\n",
        "async def download_single_transcript(url: str, ticker: str, title: str = None):\n",
        "    \"\"\"\n",
        "    ä¸‹è½½å•ä¸ª URL çš„ transcript åˆ°æŒ‡å®š ticker ç›®å½•\n",
        "    ä¸æ‰¹é‡ä¸‹è½½ä½¿ç”¨ç›¸åŒçš„å¤„ç†é€»è¾‘\n",
        "    \n",
        "    Args:\n",
        "        url: Seeking Alpha çš„ transcript URL\n",
        "        ticker: å…¬å¸ä»£ç ï¼ˆä¾‹å¦‚ \"BK\"ï¼‰\n",
        "        title: å¯é€‰ï¼Œå¦‚æœæä¾›åˆ™ä½¿ç”¨æ­¤æ ‡é¢˜ï¼Œå¦åˆ™ä»é¡µé¢è·å–\n",
        "    \"\"\"\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"ä¸‹è½½å•ä¸ª transcript: {ticker}\")\n",
        "    print(f\"URL: {url}\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    ticker_output_dir = MAIN_OUTPUT_DIR / ticker.upper()\n",
        "    ticker_output_dir.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨ï¼ˆåœ¨è¿æ¥æµè§ˆå™¨ä¹‹å‰ï¼‰\n",
        "    if title:\n",
        "        safe_title = sanitize_filename(title)\n",
        "        txt_path = ticker_output_dir / f\"{safe_title}.txt\"\n",
        "        if txt_path.exists():\n",
        "            print(f\"âš ï¸ æ–‡ä»¶å·²å­˜åœ¨: {txt_path}\")\n",
        "            print(\"è·³è¿‡ä¸‹è½½ã€‚\")\n",
        "            return True\n",
        "    \n",
        "    # è¿æ¥æµè§ˆå™¨å¹¶æ‰§è¡Œä¸‹è½½ï¼ˆæ‰€æœ‰æ“ä½œéƒ½åœ¨ async with å—å†…ï¼‰\n",
        "    async with async_playwright() as p:\n",
        "        try:\n",
        "            browser = await p.chromium.connect_over_cdp(\"http://localhost:9222\")\n",
        "            print(\"âœ… æˆåŠŸè¿æ¥åˆ° Chrome\")\n",
        "            \n",
        "            # è·å–æˆ–åˆ›å»º context\n",
        "            if browser.contexts:\n",
        "                context = browser.contexts[0]\n",
        "            else:\n",
        "                context = await browser.new_context()\n",
        "            \n",
        "            # è·å–æˆ–åˆ›å»º page\n",
        "            if context.pages:\n",
        "                page = context.pages[0]\n",
        "                # ç¡®ä¿é¡µé¢æœ‰æ•ˆ\n",
        "                try:\n",
        "                    await page.bring_to_front()\n",
        "                except:\n",
        "                    page = await context.new_page()\n",
        "            else:\n",
        "                page = await context.new_page()\n",
        "            \n",
        "            print(\"âœ… é¡µé¢å‡†å¤‡å°±ç»ª\")\n",
        "            \n",
        "            # å¦‚æœæ²¡æœ‰æä¾› titleï¼Œä»é¡µé¢è·å–\n",
        "            if not title:\n",
        "                print(\"æ­£åœ¨è®¿é—®é¡µé¢è·å–æ ‡é¢˜...\")\n",
        "                await page.goto(url, timeout=60000, wait_until=\"domcontentloaded\")\n",
        "                await wait_for_captcha_and_resume(page)\n",
        "                await page.wait_for_load_state(\"networkidle\", timeout=30000)\n",
        "                \n",
        "                page_title = await page.title()\n",
        "                if \"Earnings Call Transcript\" in page_title:\n",
        "                    title = page_title.split(\" | \")[0] if \" | \" in page_title else page_title\n",
        "                else:\n",
        "                    title = url.split(\"/\")[-1].replace(\"-\", \" \").title()\n",
        "            \n",
        "            safe_title = sanitize_filename(title)\n",
        "            \n",
        "            # å†æ¬¡æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨\n",
        "            txt_path = ticker_output_dir / f\"{safe_title}.txt\"\n",
        "            if txt_path.exists():\n",
        "                print(f\"âš ï¸ æ–‡ä»¶å·²å­˜åœ¨: {txt_path}\")\n",
        "                print(\"è·³è¿‡ä¸‹è½½ã€‚\")\n",
        "                return True\n",
        "            \n",
        "            # ä½¿ç”¨æ‰¹é‡ä¸‹è½½çš„ç›¸åŒå‡½æ•°ä¸‹è½½ transcript\n",
        "            print(\"å¼€å§‹ä¸‹è½½ transcript...\")\n",
        "            result = await download_transcript(page, url, str(ticker_output_dir), safe_title)\n",
        "            \n",
        "            if isinstance(result, tuple):\n",
        "                _, error_message = result\n",
        "                print(f\"âŒ ä¸‹è½½å¤±è´¥: {error_message}\")\n",
        "                return False\n",
        "            else:\n",
        "                print(f\"âœ… ä¸‹è½½æˆåŠŸ: {txt_path}\")\n",
        "                return True\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"âŒ è¿æ¥æˆ–ä¸‹è½½è¿‡ç¨‹ä¸­å‡ºé”™: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            return False\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "afcb5079",
      "metadata": {},
      "outputs": [],
      "source": [
        "# å‘½å\n",
        "def sanitize_filename(title):\n",
        "    \"\"\"\n",
        "    Sanitizes a string to be a valid filename by replacing invalid characters.\n",
        "    \"\"\"\n",
        "    return re.sub(r'[<>:\"/\\\\|?*]', '_', title)\n",
        "\n",
        "\n",
        "# è§£å†³åçˆ¬éªŒè¯æœºåˆ¶\n",
        "async def wait_for_captcha_and_resume(page: Page):\n",
        "    \"\"\"\n",
        "    Checks for a captcha page and pauses the script for manual resolution.\n",
        "    \"\"\"\n",
        "    print(\"ğŸ¤– Checking for CAPTCHA...\")\n",
        "    try:\n",
        "        captcha_button = page.get_by_text(\"æŒ‰ä½\").first\n",
        "        await captcha_button.wait_for(state=\"visible\", timeout=5000)\n",
        "        \n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"âš ï¸ CAPTCHA detected! Please manually solve it in the browser.\")\n",
        "        print(\"ğŸ’¡ Once done, return to the console and press Enter to continue.\")\n",
        "        print(\"=\"*50 + \"\\n\")\n",
        "        \n",
        "        await asyncio.to_thread(input, \"Press Enter to continue...\")\n",
        "        \n",
        "        await page.wait_for_load_state('domcontentloaded')\n",
        "        print(\"âœ… CAPTCHA solved, resuming execution.\")\n",
        "        \n",
        "    except Exception:\n",
        "        print(\"ğŸš€ No CAPTCHA detected, continuing.\")\n",
        "        pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "58c87cba",
      "metadata": {},
      "outputs": [],
      "source": [
        "# è¾…åŠ©å‡½æ•°ï¼šæå–å¹´ä»½å’Œæ£€æŸ¥ç¼ºå¤±å¹´ä»½\n",
        "\n",
        "def extract_year_from_title(title: str) -> Optional[int]:\n",
        "    \"\"\"ä»æ ‡é¢˜ä¸­æå–å¹´ä»½ï¼ˆä¾‹å¦‚ \"Q1 2024 Earnings\" -> 2024ï¼‰\"\"\"\n",
        "    match = re.search(r'\\b(20\\d{2})\\b', title)\n",
        "    return int(match.group(1)) if match else None\n",
        "\n",
        "\n",
        "def check_missing_years(ticker: str, min_year: int = MIN_YEAR) -> Tuple[list, set]:\n",
        "    \"\"\"\n",
        "    æ£€æŸ¥æŸä¸ª ticker å·²ä¸‹è½½çš„æ–‡ä»¶ä¸­ç¼ºå¤±å“ªäº›å¹´ä»½\n",
        "    è¿”å›ï¼š(å·²ä¸‹è½½çš„æ–‡ä»¶åˆ—è¡¨, ç¼ºå¤±çš„å¹´ä»½é›†åˆ)\n",
        "    \"\"\"\n",
        "    ticker_dir = MAIN_OUTPUT_DIR / ticker\n",
        "    if not ticker_dir.exists():\n",
        "        return [], set(range(min_year, datetime.now().year + 1))\n",
        "    \n",
        "    # è·å–å·²ä¸‹è½½çš„æ–‡ä»¶\n",
        "    downloaded_files = list(ticker_dir.glob(\"*.txt\"))\n",
        "    years_downloaded = set()\n",
        "    \n",
        "    for file_path in downloaded_files:\n",
        "        year = extract_year_from_title(file_path.stem)\n",
        "        if year and year >= min_year:\n",
        "            years_downloaded.add(year)\n",
        "    \n",
        "    # æœŸæœ›çš„å¹´ä»½èŒƒå›´\n",
        "    current_year = datetime.now().year\n",
        "    expected_years = set(range(min_year, current_year + 1))\n",
        "    missing_years = expected_years - years_downloaded\n",
        "    \n",
        "    return downloaded_files, missing_years"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "8816b9b3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ä¸‹è½½å•é¡µtranscript\n",
        "async def download_transcript(page: Page, url: str, output_dir: str, title: str):\n",
        "    \"\"\"\n",
        "    Downloads a transcript using an existing Playwright page.\n",
        "    Returns True if successful, a tuple (False, error_message) otherwise.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        print(f\"Navigating to: {url}\")\n",
        "        \n",
        "        await page.goto(url, timeout=60000)\n",
        "        \n",
        "        await wait_for_captcha_and_resume(page)\n",
        "        \n",
        "        await page.wait_for_load_state(\"networkidle\")\n",
        "\n",
        "        # Use a more specific locator to avoid \"strict mode\" errors\n",
        "        transcript_locator = 'div.T2G6W[data-test-id=\"content-container\"]'\n",
        "        \n",
        "        error_page_check = page.locator('h1[data-test-id=\"yikes-page-title\"]')\n",
        "        if await error_page_check.is_visible():\n",
        "            raise Exception(\"Page not found on Seeking Alpha.\")\n",
        "        \n",
        "        await page.wait_for_selector(transcript_locator, timeout=10000)\n",
        "        \n",
        "        transcript_div = page.locator(transcript_locator)\n",
        "        transcript_text = await transcript_div.inner_text()\n",
        "        \n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        \n",
        "        file_path = os.path.join(output_dir, f\"{title}.txt\")\n",
        "        with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(transcript_text)\n",
        "        \n",
        "        print(f\"âœ… Transcript saved to {file_path}\")\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        # Catch any exception and return it as a failure\n",
        "        return False, str(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "8fbe6b6d",
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_year_from_title(title: str) -> Optional[int]:\n",
        "    \"\"\"ä»æ ‡é¢˜ä¸­æå–å¹´ä»½\"\"\"\n",
        "    match = re.search(r'\\b(20\\d{2})\\b', title)\n",
        "    return int(match.group(1)) if match else None\n",
        "\n",
        "\n",
        "def check_missing_years(ticker: str, min_year: int = MIN_YEAR) -> Tuple[list, set]:\n",
        "    \"\"\"\n",
        "    æ£€æŸ¥æŸä¸ª ticker å·²ä¸‹è½½çš„æ–‡ä»¶ä¸­ç¼ºå¤±å“ªäº›å¹´ä»½\n",
        "    è¿”å›ï¼š(éœ€è¦ä¸‹è½½çš„ URL åˆ—è¡¨, ç¼ºå¤±çš„å¹´ä»½é›†åˆ)\n",
        "    \"\"\"\n",
        "    ticker_dir = MAIN_OUTPUT_DIR / ticker\n",
        "    if not ticker_dir.exists():\n",
        "        return [], set(range(min_year, datetime.now().year + 1))\n",
        "    \n",
        "    # è·å–å·²ä¸‹è½½çš„æ–‡ä»¶\n",
        "    downloaded_files = list(ticker_dir.glob(\"*.txt\"))\n",
        "    years_downloaded = set()\n",
        "    \n",
        "    for file_path in downloaded_files:\n",
        "        year = extract_year_from_title(file_path.stem)\n",
        "        if year and year >= min_year:\n",
        "            years_downloaded.add(year)\n",
        "    \n",
        "    # æœŸæœ›çš„å¹´ä»½èŒƒå›´\n",
        "    current_year = datetime.now().year\n",
        "    expected_years = set(range(min_year, current_year + 1))\n",
        "    missing_years = expected_years - years_downloaded\n",
        "    \n",
        "    return downloaded_files, missing_years\n",
        "\n",
        "\n",
        "# ä¸»æµç¨‹\n",
        "async def main(ticker_list, min_year: int = MIN_YEAR, check_missing: bool = True):\n",
        "    \"\"\"\n",
        "    ä¸‹è½½æ‰€æœ‰ ticker çš„ transcript\n",
        "    \n",
        "    Args:\n",
        "        ticker_list: è¦å¤„ç†çš„ ticker åˆ—è¡¨\n",
        "        min_year: æœ€å°å¹´ä»½ï¼ˆåªä¸‹è½½æ­¤å¹´ä»½åŠä»¥åçš„æ•°æ®ï¼‰\n",
        "        check_missing: æ˜¯å¦æ£€æŸ¥ç¼ºå¤±å¹´ä»½å¹¶é‡æ–°ä¸‹è½½\n",
        "    \"\"\"\n",
        "    print(\"=\" * 60)\n",
        "    print(\"å¼€å§‹ä¸‹è½½ Earnings Call Transcript\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"ç›®æ ‡å…¬å¸: {ticker_list}\")\n",
        "    print(f\"æ—¶é—´èŒƒå›´: {min_year}å¹´åŠä»¥å\")\n",
        "    print(f\"Chrome ç«¯å£: 9222\")\n",
        "    print(\"\\nè¯·ç¡®ä¿å·²è¿è¡Œä»¥ä¸‹å‘½ä»¤å¯åŠ¨ Chrome:\")\n",
        "    print(\"nohup /Applications/Google\\\\ Chrome.app/Contents/MacOS/Google\\\\ Chrome \\\\\")\n",
        "    print(\"  --remote-debugging-port=9222 \\\\\")\n",
        "    print(\"  --user-data-dir=/tmp/chrome_sa \\\\\")\n",
        "    print(\"  >/tmp/chrome_sa.log 2>&1 &\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    json_files = []\n",
        "    for ticker in ticker_list:\n",
        "        json_path = JSON_DIR / f\"{ticker}.json\"\n",
        "        if json_path.exists() and json_path.stat().st_size > 0:\n",
        "            json_files.append((ticker, json_path))\n",
        "        else:\n",
        "            print(f\"âš ï¸ JSON not found for {ticker}, skipping.\")\n",
        "\n",
        "    async with async_playwright() as p:\n",
        "        try:\n",
        "            browser = await p.chromium.connect_over_cdp(\"http://localhost:9222\")\n",
        "            context = browser.contexts[0]\n",
        "            page = context.pages[0] if context.pages else await context.new_page()\n",
        "            print(\"âœ… æˆåŠŸè¿æ¥åˆ° Chrome\")\n",
        "        except Exception as e:\n",
        "            print(f\"â— Failed to connect to browser instance: {e}\")\n",
        "            print(\"Please ensure you have run Chrome with --remote-debugging-port=9222\")\n",
        "            return\n",
        "\n",
        "        for ticker, json_path in json_files:\n",
        "            try:\n",
        "                if json_path.stat().st_size == 0:\n",
        "                    print(f\"â— Skipping empty file: {json_path.name}\")\n",
        "                    continue\n",
        "\n",
        "                ticker_output_dir = MAIN_OUTPUT_DIR / ticker\n",
        "                ticker_output_dir.mkdir(parents=True, exist_ok=True)\n",
        "                \n",
        "                # æ£€æŸ¥ç¼ºå¤±å¹´ä»½\n",
        "                downloaded_files, missing_years = check_missing_years(ticker, min_year)\n",
        "                print(f\"\\n{'='*60}\")\n",
        "                print(f\"--- Processing Ticker: {ticker} ---\")\n",
        "                print(f\"å·²ä¸‹è½½æ–‡ä»¶æ•°: {len(downloaded_files)}\")\n",
        "                if missing_years:\n",
        "                    print(f\"âš ï¸ ç¼ºå¤±å¹´ä»½: {sorted(missing_years)}\")\n",
        "                else:\n",
        "                    print(f\"âœ… æ‰€æœ‰å¹´ä»½ï¼ˆ{min_year}å¹´åŠä»¥åï¼‰éƒ½å·²ä¸‹è½½\")\n",
        "                \n",
        "                with open(json_path, 'r', encoding='utf-8') as f:\n",
        "                    data = json.load(f)\n",
        "                \n",
        "                # è¿‡æ»¤ï¼šåªå¤„ç†æŒ‡å®šå¹´ä»½åŠä»¥åçš„æ•°æ®\n",
        "                results = []\n",
        "                for item in data:\n",
        "                    title = item.get('title', '')\n",
        "                    link = item.get('link', '')\n",
        "                    if not title or not link:\n",
        "                        continue\n",
        "                    \n",
        "                    year = extract_year_from_title(title)\n",
        "                    if year is None or year < min_year:\n",
        "                        continue\n",
        "                    \n",
        "                    # å¦‚æœæ£€æŸ¥ç¼ºå¤±å¹´ä»½ï¼Œåªä¸‹è½½ç¼ºå¤±å¹´ä»½çš„æ•°æ®\n",
        "                    if check_missing and missing_years and year not in missing_years:\n",
        "                        continue\n",
        "                    \n",
        "                    results.append((title, link))\n",
        "                \n",
        "                print(f\"éœ€è¦ä¸‹è½½çš„ URL æ•°: {len(results)}\")\n",
        "                \n",
        "                # å®šä¹‰æ¯ä¸ªå…¬å¸çš„å¤±è´¥æ—¥å¿—æ–‡ä»¶è·¯å¾„\n",
        "                failed_links_file = ticker_output_dir / \"failed.json\"\n",
        "                failed_links = []\n",
        "                \n",
        "                # è·å–å·²ä¸‹è½½çš„æ–‡ä»¶åé›†åˆï¼ˆç”¨äºå¿«é€Ÿæ£€æŸ¥ï¼‰\n",
        "                downloaded_filenames = {f.stem for f in downloaded_files}\n",
        "                \n",
        "                for title, link in results:\n",
        "                    safe_title = sanitize_filename(title)\n",
        "                    \n",
        "                    # æ£€æŸ¥æ˜¯å¦å·²ä¸‹è½½\n",
        "                    if safe_title in downloaded_filenames:\n",
        "                        print(f\"âœ… Transcript for '{title[:60]}...' already exists. Skipping.\")\n",
        "                        continue\n",
        "                    \n",
        "                    clean_url = link.split('#')[0]\n",
        "                    if 'Earnings' in title:\n",
        "                        download_result = await download_transcript(page, clean_url, str(ticker_output_dir), safe_title)\n",
        "                        \n",
        "                        # æ£€æŸ¥ä¸‹è½½ç»“æœï¼Œå¦‚æœå¤±è´¥åˆ™æ·»åŠ åˆ°åˆ—è¡¨ä¸­\n",
        "                        if isinstance(download_result, tuple):\n",
        "                            _, error_message = download_result\n",
        "                            failed_links.append({\n",
        "                                \"title\": title,\n",
        "                                \"link\": clean_url,\n",
        "                                \"error\": error_message\n",
        "                            })\n",
        "                        \n",
        "                        delay = random.uniform(2, 5)\n",
        "                        print(f\"Pausing for {delay:.2f} seconds...\")\n",
        "                        await asyncio.sleep(delay)\n",
        "                \n",
        "                # å¾ªç¯ç»“æŸåï¼Œå°†è¯¥å…¬å¸çš„å¤±è´¥é“¾æ¥ä¿å­˜åˆ°å…¶ä¸“å±çš„ failed.json æ–‡ä»¶\n",
        "                if failed_links:\n",
        "                    with open(failed_links_file, 'w', encoding='utf-8') as f:\n",
        "                        json.dump(failed_links, f, indent=4, ensure_ascii=False)\n",
        "                    print(f\"â— {len(failed_links)} failed links saved to {failed_links_file}\")\n",
        "                \n",
        "                # æœ€ç»ˆç»Ÿè®¡\n",
        "                final_files = list(ticker_output_dir.glob(\"*.txt\"))\n",
        "                final_years = {}\n",
        "                for f in final_files:\n",
        "                    year = extract_year_from_title(f.stem)\n",
        "                    if year:\n",
        "                        final_years[year] = final_years.get(year, 0) + 1\n",
        "                print(f\"âœ… {ticker} å®Œæˆï¼æœ€ç»ˆæ–‡ä»¶æ•°: {len(final_files)}\")\n",
        "                print(f\"   å¹´ä»½åˆ†å¸ƒ: {dict(sorted(final_years.items()))}\")\n",
        "\n",
        "            except json.JSONDecodeError:\n",
        "                print(f\"â— Error: {json_path.name} is not a valid JSON file. Skipping.\")\n",
        "                continue\n",
        "            except Exception as e:\n",
        "                print(f\"â— An unexpected error occurred while processing {ticker}: {e}\")\n",
        "                import traceback\n",
        "                traceback.print_exc()\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"--- All done. ---\")\n",
        "    print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "810c38b7",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "JSONç›®å½•: /Users/xinyuewang/Desktop/1.27/data/transcript_index\n",
            "Transcriptç›®å½•: /Users/xinyuewang/Desktop/1.27/data/transcripts\n",
            "ç›®æ ‡å…¬å¸: ['JPM', 'BK', 'DAL', 'C', 'WFC', 'BAC', 'TSM', 'MS', 'GS', 'BLK', 'MTB', 'STT', 'PNC', 'BR', 'AAPL', 'MSFT', 'AMZN', 'NVDA', 'TSLA', 'NFLX', 'AMD', 'AVGO', 'INTC', 'QCOM', 'CRM', 'ADBE', 'NOW', 'SHOP']\n"
          ]
        }
      ],
      "source": [
        "# æ˜¾ç¤ºé…ç½®ä¿¡æ¯\n",
        "print(f\"JSONç›®å½•: {JSON_DIR}\")\n",
        "print(f\"Transcriptç›®å½•: {MAIN_OUTPUT_DIR}\")\n",
        "print(f\"ç›®æ ‡å…¬å¸: {TICKER_LIST}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2df1cce1",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "3e565727",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "å¼€å§‹ä¸‹è½½ Earnings Call Transcript\n",
            "============================================================\n",
            "ç›®æ ‡å…¬å¸: ['JPM', 'BK', 'DAL', 'C', 'WFC', 'BAC', 'TSM', 'MS', 'GS', 'BLK', 'MTB', 'STT', 'PNC', 'BR', 'AAPL', 'MSFT', 'AMZN', 'NVDA', 'TSLA', 'NFLX', 'AMD', 'AVGO', 'INTC', 'QCOM', 'CRM', 'ADBE', 'NOW', 'SHOP']\n",
            "æ—¶é—´èŒƒå›´: 2015å¹´åŠä»¥å\n",
            "Chrome ç«¯å£: 9222\n",
            "\n",
            "è¯·ç¡®ä¿å·²è¿è¡Œä»¥ä¸‹å‘½ä»¤å¯åŠ¨ Chrome:\n",
            "nohup /Applications/Google\\ Chrome.app/Contents/MacOS/Google\\ Chrome \\\n",
            "  --remote-debugging-port=9222 \\\n",
            "  --user-data-dir=/tmp/chrome_sa \\\n",
            "  >/tmp/chrome_sa.log 2>&1 &\n",
            "============================================================\n",
            "â— Failed to connect to browser instance: BrowserType.connect_over_cdp: connect ECONNREFUSED 127.0.0.1:9222\n",
            "Call log:\n",
            "  - <ws preparing> retrieving websocket url from http://localhost:9222\n",
            "\n",
            "Please ensure you have run Chrome with --remote-debugging-port=9222\n"
          ]
        }
      ],
      "source": [
        "# ========== æ‰§è¡Œä¸‹è½½ ==========\n",
        "# è¿è¡Œæ­¤ cell æ¥ä¸‹è½½æ‰€æœ‰ 14 å®¶å…¬å¸çš„ transcript\n",
        "\n",
        "# æ­£å¸¸å®Œæ•´è·‘ä¸€éï¼ˆæŒ‰ç¼ºå¤±å¹´ä»½è¡¥é½ï¼‰\n",
        "await main(TICKER_LIST, min_year=2015, check_missing=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "026829df",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========== å¯é€‰å·¥å…·ï¼šæ¸…ç†å·²ç»æˆåŠŸä¸‹è½½å¯¹åº”æ–‡ä»¶çš„ failed.json ==========\n",
        "\n",
        "import json\n",
        "\n",
        "\n",
        "def clean_failed_files(tickers=None):\n",
        "    \"\"\"æ ¹æ®å½“å‰å·²ç»å­˜åœ¨çš„ .txt æ–‡ä»¶ï¼Œæ¸…ç†å„ ticker ç›®å½•ä¸‹çš„ failed.jsonã€‚\n",
        "    - å¦‚æœæŸæ¡ failed è®°å½•å¯¹åº”çš„ txt å·²ç»ä¸‹è½½æˆåŠŸï¼Œåˆ™ä» failed.json ä¸­åˆ æ‰å®ƒ\n",
        "    - å¦‚æœåˆ å®Œä¹‹ååˆ—è¡¨ä¸ºç©ºï¼Œåˆ™ç›´æ¥åˆ é™¤æ•´ä¸ª failed.json æ–‡ä»¶\n",
        "    \"\"\"\n",
        "    if tickers is None:\n",
        "        tickers = TICKER_LIST\n",
        "\n",
        "    for ticker in tickers:\n",
        "        ticker_dir = MAIN_OUTPUT_DIR / ticker\n",
        "        failed_path = ticker_dir / \"failed.json\"\n",
        "        if not failed_path.exists():\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            with open(failed_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                failed_links = json.load(f)\n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸ {ticker}: failed.json è¯»å–å¤±è´¥ï¼Œè·³è¿‡ã€‚é”™è¯¯: {e}\")\n",
        "            continue\n",
        "\n",
        "        new_failed = []\n",
        "        for item in failed_links:\n",
        "            title = item.get(\"title\", \"\")\n",
        "            safe_title = sanitize_filename(title)\n",
        "            txt_path = ticker_dir / f\"{safe_title}.txt\"\n",
        "            # åªæœ‰è¿˜æ²¡æˆåŠŸä¸‹è½½å¯¹åº” txt çš„ï¼Œæ‰ä¿ç•™åœ¨ failed é‡Œ\n",
        "            if not txt_path.exists():\n",
        "                new_failed.append(item)\n",
        "\n",
        "        if new_failed:\n",
        "            with open(failed_path, \"w\", encoding=\"utf-8\") as f:\n",
        "                json.dump(new_failed, f, indent=4, ensure_ascii=False)\n",
        "            print(f\"â— {ticker}: ä»æœ‰ {len(new_failed)} æ¡é“¾æ¥å¤±è´¥ï¼Œå·²æ›´æ–° failed.json\")\n",
        "        else:\n",
        "            failed_path.unlink()\n",
        "            print(f\"âœ… {ticker}: æ‰€æœ‰ä¹‹å‰å¤±è´¥çš„é“¾æ¥éƒ½å·²æˆåŠŸä¸‹è½½ï¼Œåˆ é™¤ failed.json\")\n",
        "\n",
        "\n",
        "# ç”¨æ³•ç¤ºä¾‹ï¼šåœ¨é‡æ–°è·‘è¿‡æŸäº› ticker ä¹‹åï¼Œæ¸…ç†å®ƒä»¬çš„ failed.jsonï¼š\n",
        "clean_failed_files([\"PNC\",\"MTB\", \"JPM\",\"GS\", \"C\",\"BR\",\"BLK\",\"BK\"])  # æŒ‰éœ€æ›¿æ¢ ticker åˆ—è¡¨\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "1830f635",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "å¼€å§‹ä¸‹è½½ Earnings Call Transcript\n",
            "============================================================\n",
            "ç›®æ ‡å…¬å¸: ['STT', 'PNC', 'MTB', 'JPM', 'GS', 'C', 'BR', 'BLK', 'BK']\n",
            "æ—¶é—´èŒƒå›´: 2015å¹´åŠä»¥å\n",
            "Chrome ç«¯å£: 9222\n",
            "\n",
            "è¯·ç¡®ä¿å·²è¿è¡Œä»¥ä¸‹å‘½ä»¤å¯åŠ¨ Chrome:\n",
            "nohup /Applications/Google\\ Chrome.app/Contents/MacOS/Google\\ Chrome \\\n",
            "  --remote-debugging-port=9222 \\\n",
            "  --user-data-dir=/tmp/chrome_sa \\\n",
            "  >/tmp/chrome_sa.log 2>&1 &\n",
            "============================================================\n",
            "â— Failed to connect to browser instance: BrowserType.connect_over_cdp: connect ECONNREFUSED 127.0.0.1:9222\n",
            "Call log:\n",
            "  - <ws preparing> retrieving websocket url from http://localhost:9222\n",
            "\n",
            "Please ensure you have run Chrome with --remote-debugging-port=9222\n"
          ]
        }
      ],
      "source": [
        "# å¦‚æœæŸäº›å…¬å¸æœ‰ failed.jsonï¼Œæƒ³å•ç‹¬é‡è¯•å®ƒä»¬ï¼Œå¯ä»¥æŠŠä¸‹é¢çš„ç¤ºä¾‹è§£æ³¨é‡Šï¼š\n",
        "# æ¯”å¦‚åªé‡è¯• MS å’Œ WFCï¼š\n",
        "failed_tickers = [\"STT\", \"PNC\",\"MTB\", \"JPM\",\"GS\", \"C\",\"BR\",\"BLK\",\"BK\",]\n",
        "await main(failed_tickers, min_year=MIN_YEAR, check_missing=False)  # ä¸æŒ‰å¹´ä»½è·³è¿‡ï¼Œé‡æ–°å°è¯•æ‰€æœ‰é“¾æ¥"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
