{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "2a77a91b",
      "metadata": {},
      "source": [
        "# 3.4 Sentiment Scores\n",
        "\n",
        "从 `earnings_calls.db` 读取 segment 的 content，使用 **Loughran-McDonald (LM) 词典** 计算情感特征，追加到 `earnings_calls_features.db` 的 `segments_features` 表。\n",
        "\n",
        "**实现**：正则分词，LM 词典 `data/LM/LM_MasterDictionary.csv`。\n",
        "\n",
        "**依赖**：若 `segments_features` 已存在（3.2/3.3）则追加 LM；否则从 segments 创建。\n",
        "\n",
        "**特征**：\n",
        "- lm_positive, lm_negative, lm_uncertainty, lm_litigious\n",
        "- lm_modal_weak, lm_modal_strong, lm_constraining, lm_complexity\n",
        "- lm_net_sentiment = pos − neg\n",
        "- lm_polarity = (pos − neg) / (pos + neg + ε)\n",
        "- lm_subjectivity = pos + neg + unc + lit\n",
        "- distilbert_sentiment_score（DistilBERT 情感分数，POS 为正，NEG 为负）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "24354485",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SOURCE_DB: /Users/xinyuewang/Desktop/1.27/data/earnings_calls.db\n",
            "OUTPUT_DB: /Users/xinyuewang/Desktop/1.27/data/earnings_calls_features.db\n",
            "LM_CSV_PATH: /Users/xinyuewang/Desktop/1.27/data/LM/LM_MasterDictionary.csv\n"
          ]
        }
      ],
      "source": [
        "# ========== 配置 ==========\n",
        "\n",
        "import re\n",
        "import sqlite3\n",
        "from pathlib import Path\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "PROJECT_ROOT = Path(\"..\").resolve()\n",
        "SOURCE_DB = PROJECT_ROOT / \"data\" / \"earnings_calls.db\"\n",
        "OUTPUT_DB = PROJECT_ROOT / \"data\" / \"earnings_calls_features.db\"\n",
        "LM_CSV_PATH = PROJECT_ROOT / \"data\" / \"LM\" / \"LM_MasterDictionary.csv\"\n",
        "\n",
        "EPSILON = 1e-10\n",
        "\n",
        "print(\"SOURCE_DB:\", SOURCE_DB)\n",
        "print(\"OUTPUT_DB:\", OUTPUT_DB)\n",
        "print(\"LM_CSV_PATH:\", LM_CSV_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "b1be1519",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LM 词典加载完成: Positive=347, Negative=2345, ...\n"
          ]
        }
      ],
      "source": [
        "# ========== 1. 加载 LM 词典 & 正则分词 ==========\n",
        "\n",
        "WORD_PATTERN = re.compile(r\"[A-Za-z']+\")\n",
        "\n",
        "\n",
        "def tokenize_words(text: str):\n",
        "    return [m.group(0).lower() for m in WORD_PATTERN.finditer(text or \"\")]\n",
        "\n",
        "\n",
        "def build_lm_set(lm_df, col: str):\n",
        "    if col not in lm_df.columns:\n",
        "        return set()\n",
        "    return set(lm_df.loc[lm_df[col] > 0, \"Word\"].astype(str).str.lower())\n",
        "\n",
        "\n",
        "lm_df = pd.read_csv(LM_CSV_PATH)\n",
        "lm_df[\"Word\"] = lm_df[\"Word\"].astype(str).str.lower()\n",
        "\n",
        "LM_POSITIVE = build_lm_set(lm_df, \"Positive\")\n",
        "LM_NEGATIVE = build_lm_set(lm_df, \"Negative\")\n",
        "LM_UNCERTAINTY = build_lm_set(lm_df, \"Uncertainty\")\n",
        "LM_LITIGIOUS = build_lm_set(lm_df, \"Litigious\")\n",
        "LM_MODAL_WEAK = build_lm_set(lm_df, \"Weak_Modal\")\n",
        "LM_MODAL_STRONG = build_lm_set(lm_df, \"Strong_Modal\")\n",
        "LM_CONSTRAINING = build_lm_set(lm_df, \"Constraining\")\n",
        "LM_COMPLEXITY = build_lm_set(lm_df, \"Complexity\")\n",
        "\n",
        "print(f\"LM 词典加载完成: Positive={len(LM_POSITIVE)}, Negative={len(LM_NEGATIVE)}, ...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "f39b1e87",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========== 2. LM 情感特征计算函数 ==========\n",
        "\n",
        "def compute_lm_features(text: str) -> dict:\n",
        "    tokens = tokenize_words(text)\n",
        "    n = len(tokens) if tokens else 1\n",
        "\n",
        "    def ratio(s):\n",
        "        return sum(1 for w in tokens if w in s) / n\n",
        "\n",
        "    pos = ratio(LM_POSITIVE)\n",
        "    neg = ratio(LM_NEGATIVE)\n",
        "    unc = ratio(LM_UNCERTAINTY)\n",
        "    lit = ratio(LM_LITIGIOUS)\n",
        "    mweak = ratio(LM_MODAL_WEAK)\n",
        "    mstrong = ratio(LM_MODAL_STRONG)\n",
        "    constr = ratio(LM_CONSTRAINING)\n",
        "    complx = ratio(LM_COMPLEXITY)\n",
        "\n",
        "    net_sentiment = pos - neg\n",
        "    polarity = (pos - neg) / (pos + neg + EPSILON)\n",
        "    subjectivity = pos + neg + unc + lit\n",
        "\n",
        "    return {\n",
        "        \"lm_positive\": pos,\n",
        "        \"lm_negative\": neg,\n",
        "        \"lm_uncertainty\": unc,\n",
        "        \"lm_litigious\": lit,\n",
        "        \"lm_modal_weak\": mweak,\n",
        "        \"lm_modal_strong\": mstrong,\n",
        "        \"lm_constraining\": constr,\n",
        "        \"lm_complexity\": complx,\n",
        "        \"lm_net_sentiment\": net_sentiment,\n",
        "        \"lm_polarity\": polarity,\n",
        "        \"lm_subjectivity\": subjectivity,\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "1c93465c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "segments_features 已存在，读取 2374 行，追加 LM 特征\n",
            "合并 content 后: 2374 行\n"
          ]
        }
      ],
      "source": [
        "# ========== 3. 读取数据：若有 segments_features 则追加 LM，否则从 segments 创建 ==========\n",
        "\n",
        "conn_src = sqlite3.connect(SOURCE_DB)\n",
        "df_segments_full = pd.read_sql_query(\n",
        "    \"SELECT id, ticker, quarter, section, timestamp, url, source_file, content FROM segments\",\n",
        "    conn_src\n",
        ")\n",
        "conn_src.close()\n",
        "\n",
        "conn_out = sqlite3.connect(OUTPUT_DB)\n",
        "cur = conn_out.cursor()\n",
        "cur.execute(\"SELECT name FROM sqlite_master WHERE type='table' AND name='segments_features'\")\n",
        "table_exists = cur.fetchone() is not None\n",
        "conn_out.close()\n",
        "\n",
        "if table_exists:\n",
        "    conn_out = sqlite3.connect(OUTPUT_DB)\n",
        "    df_features = pd.read_sql_query(\"SELECT * FROM segments_features\", conn_out)\n",
        "    conn_out.close()\n",
        "    df_merged = df_features.merge(df_segments_full[[\"id\", \"content\"]], on=\"id\", how=\"left\")\n",
        "    print(f\"segments_features 已存在，读取 {len(df_features)} 行，追加 LM 特征\")\n",
        "else:\n",
        "    df_merged = df_segments_full.copy()\n",
        "    print(f\"segments_features 不存在，从 segments 创建基础表 + LM 特征（{len(df_merged)} 行）\")\n",
        "\n",
        "print(f\"合并 content 后: {len(df_merged)} 行\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "bb031864",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "已计算 200 / 2374\n",
            "已计算 400 / 2374\n",
            "已计算 600 / 2374\n",
            "已计算 800 / 2374\n",
            "已计算 1000 / 2374\n",
            "已计算 1200 / 2374\n",
            "已计算 1400 / 2374\n",
            "已计算 1600 / 2374\n",
            "已计算 1800 / 2374\n",
            "已计算 2000 / 2374\n",
            "已计算 2200 / 2374\n",
            "合并完成，共 2374 行\n"
          ]
        }
      ],
      "source": [
        "# ========== 4. 计算 LM 特征并追加列 ==========\n",
        "\n",
        "lm_rows = []\n",
        "for idx, row in df_merged.iterrows():\n",
        "    feat = compute_lm_features(row[\"content\"])\n",
        "    lm_rows.append(feat)\n",
        "    if (idx + 1) % 200 == 0:\n",
        "        print(f\"已计算 {idx + 1} / {len(df_merged)}\")\n",
        "\n",
        "df_lm = pd.DataFrame(lm_rows)\n",
        "df_merged = df_merged.drop(columns=[\"content\"])\n",
        "df_out = pd.concat([df_merged, df_lm], axis=1)\n",
        "print(f\"合并完成，共 {len(df_out)} 行\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "614600dc",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "已更新 /Users/xinyuewang/Desktop/1.27/data/earnings_calls_features.db\n",
            "表 segments_features: 2374 行，31 列\n"
          ]
        }
      ],
      "source": [
        "# ========== 5. 写回 earnings_calls_features.db ==========\n",
        "\n",
        "conn_out = sqlite3.connect(OUTPUT_DB)\n",
        "df_out.to_sql(\"segments_features\", conn_out, if_exists=\"replace\", index=False)\n",
        "conn_out.close()\n",
        "\n",
        "print(f\"已更新 {OUTPUT_DB}\")\n",
        "print(f\"表 segments_features: {len(df_out)} 行，{len(df_out.columns)} 列\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "f8fbfcf7",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>ticker</th>\n",
              "      <th>quarter</th>\n",
              "      <th>section</th>\n",
              "      <th>lm_positive</th>\n",
              "      <th>lm_negative</th>\n",
              "      <th>lm_uncertainty</th>\n",
              "      <th>lm_litigious</th>\n",
              "      <th>lm_modal_weak</th>\n",
              "      <th>lm_modal_strong</th>\n",
              "      <th>lm_constraining</th>\n",
              "      <th>lm_complexity</th>\n",
              "      <th>lm_net_sentiment</th>\n",
              "      <th>lm_polarity</th>\n",
              "      <th>lm_subjectivity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>2017-Q1</td>\n",
              "      <td>Prepared Remarks</td>\n",
              "      <td>0.025591</td>\n",
              "      <td>0.003563</td>\n",
              "      <td>0.003563</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.002592</td>\n",
              "      <td>0.004535</td>\n",
              "      <td>0.001620</td>\n",
              "      <td>0.001944</td>\n",
              "      <td>0.022028</td>\n",
              "      <td>0.755556</td>\n",
              "      <td>0.032718</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>2017-Q1</td>\n",
              "      <td>Q&amp;A</td>\n",
              "      <td>0.016356</td>\n",
              "      <td>0.009226</td>\n",
              "      <td>0.007968</td>\n",
              "      <td>0.001887</td>\n",
              "      <td>0.005452</td>\n",
              "      <td>0.004403</td>\n",
              "      <td>0.000839</td>\n",
              "      <td>0.001887</td>\n",
              "      <td>0.007129</td>\n",
              "      <td>0.278689</td>\n",
              "      <td>0.035437</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>2017-Q2</td>\n",
              "      <td>Prepared Remarks</td>\n",
              "      <td>0.026176</td>\n",
              "      <td>0.002353</td>\n",
              "      <td>0.004412</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.003235</td>\n",
              "      <td>0.003235</td>\n",
              "      <td>0.001471</td>\n",
              "      <td>0.003529</td>\n",
              "      <td>0.023824</td>\n",
              "      <td>0.835052</td>\n",
              "      <td>0.032941</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>2017-Q2</td>\n",
              "      <td>Q&amp;A</td>\n",
              "      <td>0.010514</td>\n",
              "      <td>0.008457</td>\n",
              "      <td>0.011657</td>\n",
              "      <td>0.001829</td>\n",
              "      <td>0.006171</td>\n",
              "      <td>0.003657</td>\n",
              "      <td>0.000914</td>\n",
              "      <td>0.000686</td>\n",
              "      <td>0.002057</td>\n",
              "      <td>0.108434</td>\n",
              "      <td>0.032457</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>AAPL</td>\n",
              "      <td>2018-Q2</td>\n",
              "      <td>Prepared Remarks</td>\n",
              "      <td>0.021046</td>\n",
              "      <td>0.002631</td>\n",
              "      <td>0.007600</td>\n",
              "      <td>0.000292</td>\n",
              "      <td>0.004385</td>\n",
              "      <td>0.006139</td>\n",
              "      <td>0.000877</td>\n",
              "      <td>0.002923</td>\n",
              "      <td>0.018416</td>\n",
              "      <td>0.777778</td>\n",
              "      <td>0.031570</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id ticker  quarter           section  lm_positive  lm_negative  \\\n",
              "0   1   AAPL  2017-Q1  Prepared Remarks     0.025591     0.003563   \n",
              "1   2   AAPL  2017-Q1               Q&A     0.016356     0.009226   \n",
              "2   3   AAPL  2017-Q2  Prepared Remarks     0.026176     0.002353   \n",
              "3   4   AAPL  2017-Q2               Q&A     0.010514     0.008457   \n",
              "4   5   AAPL  2018-Q2  Prepared Remarks     0.021046     0.002631   \n",
              "\n",
              "   lm_uncertainty  lm_litigious  lm_modal_weak  lm_modal_strong  \\\n",
              "0        0.003563      0.000000       0.002592         0.004535   \n",
              "1        0.007968      0.001887       0.005452         0.004403   \n",
              "2        0.004412      0.000000       0.003235         0.003235   \n",
              "3        0.011657      0.001829       0.006171         0.003657   \n",
              "4        0.007600      0.000292       0.004385         0.006139   \n",
              "\n",
              "   lm_constraining  lm_complexity  lm_net_sentiment  lm_polarity  \\\n",
              "0         0.001620       0.001944          0.022028     0.755556   \n",
              "1         0.000839       0.001887          0.007129     0.278689   \n",
              "2         0.001471       0.003529          0.023824     0.835052   \n",
              "3         0.000914       0.000686          0.002057     0.108434   \n",
              "4         0.000877       0.002923          0.018416     0.777778   \n",
              "\n",
              "   lm_subjectivity  \n",
              "0         0.032718  \n",
              "1         0.035437  \n",
              "2         0.032941  \n",
              "3         0.032457  \n",
              "4         0.031570  "
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# ========== 6. 预览 ==========\n",
        "\n",
        "lm_cols = [c for c in df_out.columns if c.startswith(\"lm_\")]\n",
        "conn = sqlite3.connect(OUTPUT_DB)\n",
        "preview = pd.read_sql_query(\n",
        "    f\"SELECT id, ticker, quarter, section, {', '.join(lm_cols)} FROM segments_features LIMIT 5\",\n",
        "    conn\n",
        ")\n",
        "conn.close()\n",
        "preview"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "f5f97aa8",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b0d38ebf2a954104ab16df490aeb05e0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d12efe1b81a446a6b6813f9fd03ea90a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "48f5c18861ae40f3a0b46268acc1b3a4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading weights:   0%|          | 0/104 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e82a2f0060e445c7a343f27217a592fa",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "07eaa7dd028849bf91dab8f7708dfe9a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "# ========== 7. 覆盖版：新增 bert_sentiment_mean（句子级 POS-NEG 均值） ==========\n",
        "# 从这里直接运行即可（不依赖上面旧的 DistilBERT cell）\n",
        "\n",
        "import re\n",
        "import sqlite3\n",
        "from pathlib import Path\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "PROJECT_ROOT = Path(\"..\").resolve()\n",
        "SOURCE_DB = PROJECT_ROOT / \"data\" / \"earnings_calls.db\"\n",
        "OUTPUT_DB = PROJECT_ROOT / \"data\" / \"earnings_calls_features.db\"\n",
        "\n",
        "DISTILBERT_MODEL = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "DISTILBERT_BATCH_SIZE = 32\n",
        "DISTILBERT_MAX_LENGTH = 512\n",
        "SENT_SPLIT_PATTERN = re.compile(r\"(?<=[.!?])\\s+\")\n",
        "\n",
        "\n",
        "def split_sentences(text: str):\n",
        "    text = (text or \"\").strip()\n",
        "    if not text:\n",
        "        return []\n",
        "    sents = [s.strip() for s in SENT_SPLIT_PATTERN.split(text) if s.strip()]\n",
        "    return sents if sents else [text]\n",
        "\n",
        "\n",
        "def compute_bert_sentiment_mean(texts):\n",
        "    try:\n",
        "        from transformers import pipeline\n",
        "    except Exception as e:\n",
        "        raise ImportError(\"请先安装依赖: pip install transformers torch\") from e\n",
        "\n",
        "    clf = pipeline(\n",
        "        \"sentiment-analysis\",\n",
        "        model=DISTILBERT_MODEL,\n",
        "        tokenizer=DISTILBERT_MODEL,\n",
        "    )\n",
        "\n",
        "    out_scores = []\n",
        "    total = len(texts)\n",
        "\n",
        "    for i, txt in enumerate(texts):\n",
        "        sents = split_sentences(txt)\n",
        "        if not sents:\n",
        "            out_scores.append(0.0)\n",
        "            continue\n",
        "\n",
        "        preds = clf(\n",
        "            sents,\n",
        "            batch_size=DISTILBERT_BATCH_SIZE,\n",
        "            truncation=True,\n",
        "            max_length=DISTILBERT_MAX_LENGTH,\n",
        "        )\n",
        "\n",
        "        sent_scores = []\n",
        "        for p in preds:\n",
        "            label = str(p.get(\"label\", \"\")).upper()\n",
        "            prob = float(p.get(\"score\", 0.0))\n",
        "            # POS-NEG, in [-1, 1]\n",
        "            if \"POS\" in label:\n",
        "                score = (2.0 * prob) - 1.0\n",
        "            else:\n",
        "                score = 1.0 - (2.0 * prob)\n",
        "            sent_scores.append(score)\n",
        "\n",
        "        out_scores.append(float(sum(sent_scores) / len(sent_scores)))\n",
        "\n",
        "        if (i + 1) % 100 == 0 or (i + 1) == total:\n",
        "            print(f\"bert_sentiment_mean 已计算 {i + 1} / {total}\")\n",
        "\n",
        "    return out_scores\n",
        "\n",
        "\n",
        "# 1) 读取 segments content（id + content）\n",
        "conn_src = sqlite3.connect(SOURCE_DB)\n",
        "df_segments = pd.read_sql_query(\"SELECT id, content FROM segments\", conn_src)\n",
        "conn_src.close()\n",
        "\n",
        "# 2) 读取当前 features 表\n",
        "conn_out = sqlite3.connect(OUTPUT_DB)\n",
        "df_feat = pd.read_sql_query(\"SELECT * FROM segments_features\", conn_out)\n",
        "conn_out.close()\n",
        "\n",
        "# 3) 对齐 content 并计算 bert_sentiment_mean\n",
        "df_work = df_feat.merge(df_segments, on=\"id\", how=\"left\")\n",
        "texts = df_work[\"content\"].fillna(\"\").astype(str).tolist()\n",
        "bert_mean = compute_bert_sentiment_mean(texts)\n",
        "\n",
        "if len(bert_mean) != len(df_work):\n",
        "    raise ValueError(\"bert_sentiment_mean 输出长度与样本数不一致\")\n",
        "\n",
        "# 4) 写入新列并落库（覆盖表）\n",
        "df_work[\"bert_sentiment_mean\"] = bert_mean\n",
        "df_write = df_work.drop(columns=[\"content\"], errors=\"ignore\")\n",
        "\n",
        "conn_out = sqlite3.connect(OUTPUT_DB)\n",
        "df_write.to_sql(\"segments_features\", conn_out, if_exists=\"replace\", index=False)\n",
        "conn_out.close()\n",
        "\n",
        "print(\"已写入列: bert_sentiment_mean\")\n",
        "print(\"行列:\", df_write.shape)\n",
        "\n",
        "# 5) 预览\n",
        "conn_out = sqlite3.connect(OUTPUT_DB)\n",
        "preview = pd.read_sql_query(\n",
        "    \"SELECT id, ticker, quarter, section, bert_sentiment_mean FROM segments_features LIMIT 5\",\n",
        "    conn_out,\n",
        ")\n",
        "conn_out.close()\n",
        "preview"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "afeb9b8f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-5.2.0-py3-none-any.whl.metadata (32 kB)\n",
            "Collecting torch\n",
            "  Downloading torch-2.10.0-2-cp312-none-macosx_11_0_arm64.whl.metadata (31 kB)\n",
            "Collecting huggingface-hub<2.0,>=1.3.0 (from transformers)\n",
            "  Downloading huggingface_hub-1.4.1-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (2026.1.15)\n",
            "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n",
            "  Downloading tokenizers-0.22.2-cp39-abi3-macosx_11_0_arm64.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: typer-slim in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.21.1)\n",
            "Collecting safetensors>=0.4.3 (from transformers)\n",
            "  Downloading safetensors-0.7.0-cp38-abi3-macosx_11_0_arm64.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tqdm>=4.27 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (4.11.0)\n",
            "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from torch) (75.1.0)\n",
            "Collecting sympy>=1.13.3 (from torch)\n",
            "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (2026.1.0)\n",
            "Collecting hf-xet<2.0.0,>=1.2.0 (from huggingface-hub<2.0,>=1.3.0->transformers)\n",
            "  Downloading hf_xet-1.2.0-cp37-abi3-macosx_11_0_arm64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (0.27.0)\n",
            "Requirement already satisfied: shellingham in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: click>=8.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from typer-slim->transformers) (8.3.1)\n",
            "Requirement already satisfied: anyio in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (4.2.0)\n",
            "Requirement already satisfied: certifi in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (1.0.2)\n",
            "Requirement already satisfied: idna in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (3.7)\n",
            "Requirement already satisfied: sniffio in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (1.3.0)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (0.14.0)\n",
            "Downloading transformers-5.2.0-py3-none-any.whl (10.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading torch-2.10.0-2-cp312-none-macosx_11_0_arm64.whl (79.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading huggingface_hub-1.4.1-py3-none-any.whl (553 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m553.3/553.3 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safetensors-0.7.0-cp38-abi3-macosx_11_0_arm64.whl (447 kB)\n",
            "Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.22.2-cp39-abi3-macosx_11_0_arm64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading hf_xet-1.2.0-cp37-abi3-macosx_11_0_arm64.whl (2.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sympy, safetensors, hf-xet, torch, huggingface-hub, tokenizers, transformers\n",
            "  Attempting uninstall: sympy\n",
            "    Found existing installation: sympy 1.13.2\n",
            "    Uninstalling sympy-1.13.2:\n",
            "      Successfully uninstalled sympy-1.13.2\n",
            "Successfully installed hf-xet-1.2.0 huggingface-hub-1.4.1 safetensors-0.7.0 sympy-1.14.0 tokenizers-0.22.2 torch-2.10.0 transformers-5.2.0\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install transformers torch"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
