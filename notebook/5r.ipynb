{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 5r. 单公司 2025-Q4 Beat/Miss 预测\n",
        "\n",
        "## 任务说明\n",
        "\n",
        "使用 **4.ipynb** 训练好的 Logistic 回归模型，基于 CVX 的 2025 Q1、Q2、Q3 的 earnings call transcript，预测 **2025-Q4** 是否会 Beat/Miss。\n",
        "\n",
        "- **Transcript 路径**: `预测/transcripts/CVX/`\n",
        "- **模型来源**: 4.ipynb（Logistic 回归 + StandardScaler，特征见 `data/selected_features.txt`）\n",
        "- **预测逻辑**: 当前季度 call（2025-Q3）的特征 → 预测下一季度（2025-Q4）的 Beat 概率"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "a6cd01f0",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PROJECT_ROOT: /Users/xinyuewang/Desktop/1.27\n",
            "TRANSCRIPTS_DIR: /Users/xinyuewang/Desktop/1.27/预测/transcripts/CVX\n",
            "TICKER: CVX | 预测季度: 2025-Q4\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "import pickle\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from textstat import (\n",
        "    automated_readability_index,\n",
        "    coleman_liau_index,\n",
        "    dale_chall_readability_score,\n",
        "    flesch_reading_ease,\n",
        "    flesch_kincaid_grade,\n",
        "    gunning_fog,\n",
        "    smog_index,\n",
        ")\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "PROJECT_ROOT = Path(\"..\").resolve()\n",
        "DATA_DIR = PROJECT_ROOT / \"data\"\n",
        "PRED_DIR = PROJECT_ROOT / \"预测\"\n",
        "TRANSCRIPTS_DIR = PRED_DIR / \"transcripts\" / \"CVX\"\n",
        "\n",
        "TICKER = \"CVX\"\n",
        "TARGET_QUARTERS = [\"2025-Q1\", \"2025-Q2\", \"2025-Q3\"]  # 用于特征和 delta\n",
        "PREDICT_QUARTER = \"2025-Q4\"  # 预测目标\n",
        "\n",
        "print(\"PROJECT_ROOT:\", PROJECT_ROOT)\n",
        "print(\"TRANSCRIPTS_DIR:\", TRANSCRIPTS_DIR)\n",
        "print(\"TICKER:\", TICKER, \"| 预测季度:\", PREDICT_QUARTER)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bfef8106",
      "metadata": {},
      "source": [
        "## 1. 解析与分段（与 3.ipynb 一致）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "d8c8aa7d",
      "metadata": {},
      "outputs": [],
      "source": [
        "DATE_LINE_PATTERN = re.compile(\n",
        "    r\"(?P<month>[A-Za-z]+\\.?)\\s+(?P<day>\\d{1,2}),\\s+(?P<year>\\d{4}),?\\s+\"\n",
        "    r\"(?P<hour>\\d{1,2}):(?P<minute>\\d{2})\\s*(?P<ampm>AM|PM)\\s*ET\",\n",
        "    re.IGNORECASE,\n",
        ")\n",
        "MONTH_MAP = {\n",
        "    \"jan\": 1, \"january\": 1, \"jan.\": 1,\n",
        "    \"feb\": 2, \"february\": 2, \"feb.\": 2,\n",
        "    \"mar\": 3, \"march\": 3, \"mar.\": 3,\n",
        "    \"apr\": 4, \"april\": 4, \"apr.\": 4,\n",
        "    \"may\": 5, \"jun\": 6, \"june\": 6, \"jun.\": 6,\n",
        "    \"jul\": 7, \"july\": 7, \"jul.\": 7,\n",
        "    \"aug\": 8, \"august\": 8, \"aug.\": 8,\n",
        "    \"sep\": 9, \"september\": 9, \"sep.\": 9,\n",
        "    \"oct\": 10, \"october\": 10, \"oct.\": 10,\n",
        "    \"nov\": 11, \"november\": 11, \"nov.\": 11,\n",
        "    \"dec\": 12, \"december\": 12, \"dec.\": 12,\n",
        "}\n",
        "\n",
        "QA_MARKERS = [\n",
        "    r\"Question-and-Answer Session\", r\"Questions and Answers\",\n",
        "    r\"Question and Answer Session\", r\"Q&A Session\", r\"Q & A Session\",\n",
        "]\n",
        "\n",
        "def parse_quarter_from_filename(filename: str):\n",
        "    m = re.search(r\"Q([1-4])\\s+(\\d{4})\", filename, re.IGNORECASE)\n",
        "    if m:\n",
        "        return f\"{int(m.group(2))}-Q{int(m.group(1))}\"\n",
        "    return None\n",
        "\n",
        "def parse_call_datetime_et(text: str):\n",
        "    lines = text.split(\"\\n\")[:15]\n",
        "    search_text = \"\\n\".join([l for l in lines if \"Call Start\" not in l])\n",
        "    matches = list(DATE_LINE_PATTERN.finditer(search_text))\n",
        "    valid = [(m, int(m.group(\"year\"))) for m in matches if 2000 <= int(m.group(\"year\")) <= 2030]\n",
        "    if not valid:\n",
        "        return None\n",
        "    m, _ = max(valid, key=lambda x: x[1])\n",
        "    month_str = m.group(\"month\").lower()\n",
        "    month = MONTH_MAP.get(month_str.rstrip(\".\"))\n",
        "    if month is None:\n",
        "        return None\n",
        "    day = int(m.group(\"day\")); year = int(m.group(\"year\"))\n",
        "    hour = int(m.group(\"hour\")); minute = int(m.group(\"minute\"))\n",
        "    ampm = m.group(\"ampm\").upper()\n",
        "    if ampm == \"PM\" and hour != 12: hour += 12\n",
        "    if ampm == \"AM\" and hour == 12: hour = 0\n",
        "    try:\n",
        "        return datetime(year, month, day, hour, minute).strftime(\"%Y-%m-%d %H:%M\")\n",
        "    except ValueError:\n",
        "        return None\n",
        "\n",
        "def find_qa_marker(text: str, skip_first_n_sentences: int = 10):\n",
        "    sentence_endings = re.finditer(r\"[.!?]+\\s+\", text)\n",
        "    positions = [m.end() for m in sentence_endings]\n",
        "    start = positions[skip_first_n_sentences - 1] if len(positions) >= skip_first_n_sentences else 0\n",
        "    search_text = text[start:]\n",
        "    for marker in QA_MARKERS:\n",
        "        m = re.search(marker, search_text, re.IGNORECASE)\n",
        "        if m:\n",
        "            return start + m.start()\n",
        "    return None\n",
        "\n",
        "def split_transcript_into_segments(text: str):\n",
        "    segments = []\n",
        "    qa_start = find_qa_marker(text)\n",
        "    if qa_start is not None:\n",
        "        segments.append({\"segment_type\": \"Prepared Remarks\", \"text_content\": text[:qa_start].strip()})\n",
        "        segments.append({\"segment_type\": \"Q&A\", \"text_content\": text[qa_start:].strip()})\n",
        "    else:\n",
        "        segments.append({\"segment_type\": \"Prepared Remarks\", \"text_content\": text.strip()})\n",
        "    return segments"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f85252d",
      "metadata": {},
      "source": [
        "## 2. 文本特征（与 3.ipynb 一致）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "dc8863f3",
      "metadata": {},
      "outputs": [],
      "source": [
        "PRONOUNS_ALL = {\"i\",\"me\",\"my\",\"mine\",\"you\",\"your\",\"yours\",\"he\",\"him\",\"his\",\"she\",\"her\",\"hers\",\"it\",\"its\",\"we\",\"us\",\"our\",\"ours\",\"they\",\"them\",\"their\",\"theirs\"}\n",
        "PRONOUNS_PLURAL = {\"we\",\"us\",\"our\",\"ours\",\"they\",\"them\",\"their\",\"theirs\"}\n",
        "COMMON_ADVERBS = {\"very\",\"quite\",\"rather\",\"extremely\",\"highly\",\"significantly\",\"substantially\",\"slightly\",\"barely\",\"rarely\",\"frequently\",\"usually\",\"typically\",\"generally\",\"probably\",\"possibly\",\"certainly\",\"clearly\",\"obviously\"}\n",
        "WORD_PATTERN = re.compile(r\"[A-Za-z']+\")\n",
        "\n",
        "def tokenize_words(text): return WORD_PATTERN.findall(text.lower())\n",
        "def count_sentences(text): return sum(1 for p in re.split(r\"[.!?]+\", text) if p.strip())\n",
        "\n",
        "def basic_features(text: str):\n",
        "    tokens = tokenize_words(text)\n",
        "    n_words = len(tokens)\n",
        "    n_sent = count_sentences(text)\n",
        "    avg_words = n_words / n_sent if n_sent > 0 else 0.0\n",
        "    pronouns = [w for w in tokens if w in PRONOUNS_ALL]\n",
        "    pronouns_plural = [w for w in pronouns if w in PRONOUNS_PLURAL]\n",
        "    pronoun_plural_ratio = len(pronouns_plural) / len(pronouns) if pronouns else 0.0\n",
        "    adverbs = [w for w in tokens if w in COMMON_ADVERBS or (len(w) > 3 and w.endswith(\"ly\"))]\n",
        "    adverb_ratio = len(adverbs) / n_words if n_words > 0 else 0.0\n",
        "    return {\"n_words\": n_words, \"n_sentences\": n_sent, \"avg_words_per_sentence\": avg_words, \"pronoun_plural_ratio\": pronoun_plural_ratio, \"adverb_ratio\": adverb_ratio}\n",
        "\n",
        "def readability_features(text: str):\n",
        "    out = {}\n",
        "    for name, func in [(\"ari\", automated_readability_index), (\"coleman_liau\", coleman_liau_index), (\"dale_chall\", dale_chall_readability_score), (\"flesch_ease\", flesch_reading_ease), (\"flesch_kincaid\", flesch_kincaid_grade), (\"gunning_fog\", gunning_fog), (\"smog\", smog_index)]:\n",
        "        try: out[name] = func(text)\n",
        "        except: out[name] = None\n",
        "    return out\n",
        "\n",
        "LM_CSV_PATH = DATA_DIR / \"LM\" / \"LM_MasterDictionary.csv\"\n",
        "if LM_CSV_PATH.exists():\n",
        "    lm_df = pd.read_csv(LM_CSV_PATH)\n",
        "    lm_df[\"Word\"] = lm_df[\"Word\"].astype(str).str.lower()\n",
        "    def build_lm_set(col): return set(lm_df.loc[lm_df[col] > 0, \"Word\"]) if col in lm_df.columns else set()\n",
        "    LM_POSITIVE = build_lm_set(\"Positive\"); LM_NEGATIVE = build_lm_set(\"Negative\"); LM_UNCERTAINTY = build_lm_set(\"Uncertainty\")\n",
        "    LM_LITIGIOUS = build_lm_set(\"Litigious\"); LM_SUPERFLUOUS = build_lm_set(\"Superfluous\"); LM_INTERESTING = build_lm_set(\"Interesting\")\n",
        "    LM_MODAL_WEAK = build_lm_set(\"ModalWeak\"); LM_MODAL_MODERATE = build_lm_set(\"ModalModerate\"); LM_MODAL_STRONG = build_lm_set(\"ModalStrong\")\n",
        "    LM_CONSTRAINING = build_lm_set(\"Constraining\"); LM_COMPLEXITY = build_lm_set(\"Complexity\")\n",
        "else:\n",
        "    LM_POSITIVE = LM_NEGATIVE = LM_UNCERTAINTY = LM_LITIGIOUS = LM_SUPERFLUOUS = LM_INTERESTING = set()\n",
        "    LM_MODAL_WEAK = LM_MODAL_MODERATE = LM_MODAL_STRONG = LM_CONSTRAINING = LM_COMPLEXITY = set()\n",
        "\n",
        "def lm_features(text: str):\n",
        "    tokens = tokenize_words(text)\n",
        "    n_words = len(tokens) if tokens else 1\n",
        "    def ratio(s): return sum(1 for w in tokens if w in s) / n_words\n",
        "    pos, neg = ratio(LM_POSITIVE), ratio(LM_NEGATIVE)\n",
        "    return {\"lm_positive\": pos, \"lm_negative\": neg, \"lm_uncertainty\": ratio(LM_UNCERTAINTY), \"lm_litigous\": ratio(LM_LITIGIOUS), \"lm_superfluous\": ratio(LM_SUPERFLUOUS), \"lm_interesting\": ratio(LM_INTERESTING), \"lm_modal_weak\": ratio(LM_MODAL_WEAK), \"lm_modal_moderate\": ratio(LM_MODAL_MODERATE), \"lm_modal_strong\": ratio(LM_MODAL_STRONG), \"lm_constraining\": ratio(LM_CONSTRAINING), \"lm_complexity\": ratio(LM_COMPLEXITY), \"lm_net_sentiment\": pos - neg, \"lm_polarity\": pos + neg, \"lm_subjectivity\": pos + neg + ratio(LM_UNCERTAINTY) + ratio(LM_LITIGIOUS) + ratio(LM_SUPERFLUOUS) + ratio(LM_INTERESTING)}\n",
        "\n",
        "def compute_segment_features(text: str):\n",
        "    d = {}\n",
        "    d.update(basic_features(text))\n",
        "    d.update(readability_features(text))\n",
        "    d.update(lm_features(text))\n",
        "    return d"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11a2b20a",
      "metadata": {},
      "source": [
        "## 3. 加载 CVX transcript，分段并计算特征"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "836f953a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Segment 数: 6\n",
            "  fiscal_quarter      segment_type  n_words\n",
            "0        2025-Q1  Prepared Remarks     1184\n",
            "1        2025-Q1               Q&A     6547\n",
            "2        2025-Q2  Prepared Remarks     1609\n",
            "3        2025-Q2               Q&A     5924\n",
            "4        2025-Q3  Prepared Remarks     1208\n",
            "5        2025-Q3               Q&A     6766\n"
          ]
        }
      ],
      "source": [
        "BASE_FEATURE_NAMES = [\"n_words\", \"n_sentences\", \"avg_words_per_sentence\", \"pronoun_plural_ratio\", \"adverb_ratio\", \"ari\", \"coleman_liau\", \"dale_chall\", \"flesch_ease\", \"flesch_kincaid\", \"gunning_fog\", \"smog\", \"lm_positive\", \"lm_negative\", \"lm_uncertainty\", \"lm_litigous\", \"lm_superfluous\", \"lm_interesting\", \"lm_modal_weak\", \"lm_modal_moderate\", \"lm_modal_strong\", \"lm_constraining\", \"lm_complexity\", \"lm_net_sentiment\", \"lm_polarity\", \"lm_subjectivity\"]\n",
        "\n",
        "rows = []\n",
        "for txt_path in sorted(TRANSCRIPTS_DIR.glob(\"*.txt\")):\n",
        "    quarter = parse_quarter_from_filename(txt_path.name)\n",
        "    if quarter not in TARGET_QUARTERS:\n",
        "        continue\n",
        "    text = txt_path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
        "    call_dt = parse_call_datetime_et(text)\n",
        "    for seg in split_transcript_into_segments(text):\n",
        "        feat = compute_segment_features(seg[\"text_content\"])\n",
        "        row = {\"ticker\": TICKER, \"fiscal_quarter\": quarter, \"call_datetime_et\": call_dt, \"segment_type\": seg[\"segment_type\"]}\n",
        "        row.update(feat)\n",
        "        rows.append(row)\n",
        "\n",
        "df_segments = pd.DataFrame(rows)\n",
        "print(\"Segment 数:\", len(df_segments))\n",
        "print(df_segments[[\"fiscal_quarter\", \"segment_type\", \"n_words\"]].to_string())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77d04fdf",
      "metadata": {},
      "source": [
        "## 4. 聚合为 call 级（每季度一行，_preparedremark / _QA）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "e96e56b8",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Call 数: 3\n",
            "  fiscal_quarter  n_words_preparedremark  n_words_QA\n",
            "0        2025-Q1                    1184        6547\n",
            "1        2025-Q2                    1609        5924\n",
            "2        2025-Q3                    1208        6766\n"
          ]
        }
      ],
      "source": [
        "call_rows = []\n",
        "for (ticker, fq), grp in df_segments.groupby([\"ticker\", \"fiscal_quarter\"]):\n",
        "    row = {\"ticker\": ticker, \"fiscal_quarter\": fq, \"call_datetime_et\": grp[\"call_datetime_et\"].iloc[0]}\n",
        "    prep = grp[grp[\"segment_type\"] == \"Prepared Remarks\"]\n",
        "    qa = grp[grp[\"segment_type\"] == \"Q&A\"]\n",
        "    for f in BASE_FEATURE_NAMES:\n",
        "        if f in grp.columns:\n",
        "            row[f + \"_preparedremark\"] = prep[f].iloc[0] if len(prep) > 0 else np.nan\n",
        "            row[f + \"_QA\"] = qa[f].iloc[0] if len(qa) > 0 else np.nan\n",
        "    if np.isnan(row.get(\"n_words_preparedremark\")) or row.get(\"n_words_preparedremark\") is None: row[\"n_words_preparedremark\"] = 0\n",
        "    if np.isnan(row.get(\"n_words_QA\")) or row.get(\"n_words_QA\") is None: row[\"n_words_QA\"] = 0\n",
        "    call_rows.append(row)\n",
        "\n",
        "df_calls = pd.DataFrame(call_rows).sort_values(\"fiscal_quarter\").reset_index(drop=True)\n",
        "print(\"Call 数:\", len(df_calls))\n",
        "print(df_calls[[\"fiscal_quarter\", \"n_words_preparedremark\", \"n_words_QA\"]].to_string())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3825148c",
      "metadata": {},
      "source": [
        "## 5. 添加 sentiment_divergence, qa_word_ratio, delta_net_sentiment, delta_complexity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "e28608b2",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "新特征已添加\n"
          ]
        }
      ],
      "source": [
        "df_calls[\"sentiment_divergence\"] = df_calls[\"lm_net_sentiment_preparedremark\"] - df_calls[\"lm_net_sentiment_QA\"]\n",
        "df_calls[\"qa_word_ratio\"] = df_calls[\"n_words_QA\"] / df_calls[\"n_words_preparedremark\"].replace(0, np.nan)\n",
        "\n",
        "def calculate_standardized_change(group, col_name, window=4):\n",
        "    values = group[col_name].values\n",
        "    out = np.full(len(values), np.nan)\n",
        "    for i in range(len(values)):\n",
        "        if i < window:\n",
        "            hist = values[:i]\n",
        "        else:\n",
        "            hist = values[i - window:i]\n",
        "        if len(hist) > 0 and not np.isnan(hist).all():\n",
        "            mean_val = np.nanmean(hist)\n",
        "            std_val = np.nanstd(hist)\n",
        "            if std_val > 0:\n",
        "                out[i] = (values[i] - mean_val) / std_val\n",
        "    return out\n",
        "\n",
        "df_calls[\"net_sentiment_avg\"] = (df_calls[\"lm_net_sentiment_preparedremark\"] + df_calls[\"lm_net_sentiment_QA\"]) / 2\n",
        "\n",
        "# 修复：确保 groupby().apply() 返回的是 Series，并正确对齐索引\n",
        "def compute_delta(group, col_name, window=4):\n",
        "    result = calculate_standardized_change(group, col_name, window)\n",
        "    return pd.Series(result, index=group.index)\n",
        "\n",
        "# 计算 delta_net_sentiment\n",
        "delta_net_result = df_calls.groupby(\"ticker\").apply(compute_delta, col_name=\"net_sentiment_avg\", window=4)\n",
        "# 如果返回的是 DataFrame，取第一列；如果是 Series，直接使用\n",
        "if isinstance(delta_net_result, pd.DataFrame):\n",
        "    delta_net_result = delta_net_result.iloc[:, 0]\n",
        "# 移除 groupby 的索引层级，只保留原始索引\n",
        "delta_net_result = delta_net_result.reset_index(level=0, drop=True)\n",
        "df_calls[\"delta_net_sentiment\"] = delta_net_result\n",
        "\n",
        "# 计算 delta_complexity\n",
        "delta_comp_result = df_calls.groupby(\"ticker\").apply(compute_delta, col_name=\"ari_preparedremark\", window=4)\n",
        "if isinstance(delta_comp_result, pd.DataFrame):\n",
        "    delta_comp_result = delta_comp_result.iloc[:, 0]\n",
        "delta_comp_result = delta_comp_result.reset_index(level=0, drop=True)\n",
        "df_calls[\"delta_complexity\"] = delta_comp_result\n",
        "\n",
        "df_calls = df_calls.drop(columns=[\"net_sentiment_avg\"])\n",
        "print(\"新特征已添加\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b54765f",
      "metadata": {},
      "source": [
        "## 6. 加载模型并预测 2025-Q4 Beat 概率"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "498848bd",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "CVX 2025-Q4 Beat/Miss 预测\n",
            "============================================================\n",
            "基于 2025-Q3 earnings call 特征\n",
            "预测 2025-Q4 Beat 概率: 0.5491\n",
            "预测 2025-Q4 Miss 概率: 0.4509\n",
            "结论: Beat (阈值 0.5)\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "model_path = DATA_DIR / \"model.pkl\"\n",
        "scaler_path = DATA_DIR / \"scaler.pkl\"\n",
        "selected_features_path = DATA_DIR / \"selected_features.txt\"\n",
        "\n",
        "if not model_path.exists() or not scaler_path.exists():\n",
        "    raise FileNotFoundError(\"请先运行 4.ipynb 并执行到「保存模型」的 cell，生成 data/model.pkl 和 data/scaler.pkl\")\n",
        "\n",
        "with open(model_path, \"rb\") as f:\n",
        "    model = pickle.load(f)\n",
        "with open(scaler_path, \"rb\") as f:\n",
        "    scaler = pickle.load(f)\n",
        "with open(selected_features_path) as f:\n",
        "    selected_features = [line.strip() for line in f if line.strip()]\n",
        "\n",
        "row_2025q3 = df_calls[df_calls[\"fiscal_quarter\"] == \"2025-Q3\"].iloc[0]\n",
        "X = pd.DataFrame([row_2025q3])[selected_features].fillna(0)\n",
        "X_scaled = scaler.transform(X)\n",
        "prob_beat = model.predict_proba(X_scaled)[0, 1]\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(f\"{TICKER} 2025-Q4 Beat/Miss 预测\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"基于 2025-Q3 earnings call 特征\")\n",
        "print(f\"预测 2025-Q4 Beat 概率: {prob_beat:.4f}\")\n",
        "print(f\"预测 2025-Q4 Miss 概率: {1 - prob_beat:.4f}\")\n",
        "print(f\"结论: {'Beat' if prob_beat >= 0.5 else 'Miss'} (阈值 0.5)\")\n",
        "print(\"=\" * 60)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
