{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "6c4981e5",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "é¡¹ç›®æ ¹ç›®å½•: /Users/xinyuewang/Desktop/1.27\n",
            "URLç´¢å¼•ä¿å­˜ç›®å½•: /Users/xinyuewang/Desktop/1.27/data/transcript_index\n",
            "ç›®æ ‡å…¬å¸æ•°: 14\n",
            "æ—¶é—´èŒƒå›´: 2023å¹´åŠä»¥åï¼ˆè¿‘10å¹´ï¼‰\n"
          ]
        }
      ],
      "source": [
        "# 1. è·å¾— URL - ä» Seeking Alpha è·å–æ‰€æœ‰ Earnings Call Transcript é“¾æ¥\n",
        "\n",
        "import time\n",
        "import json\n",
        "import random\n",
        "import os\n",
        "import re\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "\n",
        "import asyncio\n",
        "from playwright.async_api import async_playwright\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# ========== é…ç½®åŒº ==========\n",
        "# 14 å®¶å…¬å¸åˆ—è¡¨\n",
        "TICKER_LIST = [\n",
        "    \"JPM\",  # JPMorgan Chase\n",
        "    \"BK\",   # Bank of New York Mellon\n",
        "    \"DAL\",  # Delta Airlines\n",
        "    \"C\",    # Citigroup\n",
        "    \"WFC\",  # Wells Fargo\n",
        "    \"BAC\",  # Bank of America\n",
        "    \"TSM\",  # TSMC\n",
        "    \"MS\",   # Morgan Stanley\n",
        "    \"GS\",   # Goldman Sachs\n",
        "    \"BLK\",  # BlackRock\n",
        "    \"MTB\",  # M&T Bank\n",
        "    \"STT\",  # State Street\n",
        "    \"PNC\",  # PNC Financial\n",
        "    \"BR\",   # Broadridge Financial\n",
        "    # è¿˜éœ€è¦è¡¥é½ç¬¬15ã€16å®¶ï¼ˆæ ¹æ®å½“å‰ earnings calendarï¼‰\n",
        "]\n",
        "\n",
        "# åªè·å–è¿‘10å¹´çš„æ•°æ®ï¼ˆ2015å¹´åŠä»¥åï¼‰\n",
        "MIN_YEAR = 2023\n",
        "\n",
        "PROJECT_ROOT = Path(\"..\").resolve()\n",
        "INDEX_DIR = PROJECT_ROOT / \"data\" / \"transcript_index\"\n",
        "INDEX_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(f\"é¡¹ç›®æ ¹ç›®å½•: {PROJECT_ROOT}\")\n",
        "print(f\"URLç´¢å¼•ä¿å­˜ç›®å½•: {INDEX_DIR}\")\n",
        "print(f\"ç›®æ ‡å…¬å¸æ•°: {len(TICKER_LIST)}\")\n",
        "print(f\"æ—¶é—´èŒƒå›´: {MIN_YEAR}å¹´åŠä»¥åï¼ˆè¿‘10å¹´ï¼‰\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "764f3411",
      "metadata": {},
      "outputs": [],
      "source": [
        "#äººå·¥éªŒè¯\n",
        "async def wait_for_captcha_and_resume(page):\n",
        "    \"\"\"\n",
        "    æ£€æµ‹éªŒè¯ç é¡µé¢ï¼Œå¦‚æœå­˜åœ¨ï¼Œåˆ™æš‚åœç¨‹åºç­‰å¾…ç”¨æˆ·æ‰‹åŠ¨è§£å†³ã€‚\n",
        "    \"\"\"\n",
        "    print(\"æ­£åœ¨æ£€æŸ¥æ˜¯å¦å­˜åœ¨éªŒè¯ç ...\")\n",
        "    try:\n",
        "        captcha_button = page.get_by_text(\"æŒ‰ä½\").first\n",
        "        await captcha_button.wait_for(state=\"visible\", timeout=5000)\n",
        "\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"æ£€æµ‹åˆ°éªŒè¯ç ï¼è¯·åœ¨æµè§ˆå™¨ä¸­æ‰‹åŠ¨å®ŒæˆéªŒè¯ã€‚\")\n",
        "        print(\"å®Œæˆåï¼Œè¯·å›åˆ°æ­¤ç»ˆç«¯å¹¶æŒ‰ Enter é”®ç»§ç»­ã€‚\")\n",
        "        print(\"=\"*50 + \"\\n\")\n",
        "\n",
        "        input(\"æŒ‰ä¸‹ Enter é”®ä»¥ç»§ç»­...\")\n",
        "\n",
        "        await page.wait_for_load_state('domcontentloaded')\n",
        "        print(\"éªŒè¯ç å·²è§£å†³ï¼Œç¨‹åºç»§ç»­ã€‚\")\n",
        "\n",
        "    except Exception:\n",
        "        print(\"æœªæ£€æµ‹åˆ°éªŒè¯ç ï¼Œç»§ç»­æ‰§è¡Œã€‚\")\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "a6751300",
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_completed_tickers(progress_file=\"progress.txt\"):\n",
        "    if os.path.exists(progress_file):\n",
        "        with open(progress_file, \"r\") as f:\n",
        "            return set(line.strip() for line in f)\n",
        "    return set()\n",
        "\n",
        "def mark_as_completed(ticker, progress_file=\"progress.txt\"):\n",
        "    with open(progress_file, \"a\") as f:\n",
        "        f.write(f\"{ticker}\\n\")\n",
        "\n",
        "def mark_as_excluded(ticker, exclude_file=\"exclude_transcript.txt\"):\n",
        "    if os.path.exists(exclude_file):\n",
        "        with open(exclude_file, \"a\") as f:\n",
        "            f.write(f\"{ticker}\\n\")\n",
        "    else:\n",
        "        with open(exclude_file, \"w\") as f:\n",
        "            f.write(f\"{ticker}\\n\")\n",
        "\n",
        "def mark_as_failed(ticker, failed_file=\"failed.txt\"):\n",
        "    if os.path.exists(failed_file):\n",
        "        with open(failed_file, \"a\") as f:\n",
        "            f.write(f\"{ticker}\\n\")\n",
        "    else:\n",
        "        with open(failed_file, \"w\") as f:\n",
        "            f.write(f\"{ticker}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5700d0ac",
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_year_from_title(title: str) -> int:\n",
        "    \"\"\"ä»æ ‡é¢˜ä¸­æå–å¹´ä»½ï¼ˆä¾‹å¦‚ \"Q1 2024 Earnings\" -> 2024ï¼‰\"\"\"\n",
        "    # åŒ¹é… 4 ä½æ•°å­—å¹´ä»½\n",
        "    match = re.search(r'\\b(20\\d{2})\\b', title)\n",
        "    if match:\n",
        "        return int(match.group(1))\n",
        "    return None\n",
        "\n",
        "\n",
        "def check_missing_years(existing_results: list, min_year: int = MIN_YEAR) -> set:\n",
        "    \"\"\"æ£€æŸ¥å·²å­˜åœ¨çš„ results ä¸­ç¼ºå¤±å“ªäº›å¹´ä»½ï¼ˆ2015å¹´åŠä»¥åï¼‰\"\"\"\n",
        "    years_found = set()\n",
        "    for item in existing_results:\n",
        "        title = item.get('title', '')\n",
        "        year = extract_year_from_title(title)\n",
        "        if year and year >= min_year:\n",
        "            years_found.add(year)\n",
        "    \n",
        "    # æœŸæœ›çš„å¹´ä»½èŒƒå›´ï¼ˆå½“å‰å¹´ä»½å¾€å‰æ¨10å¹´ï¼‰\n",
        "    current_year = datetime.now().year\n",
        "    expected_years = set(range(min_year, current_year + 1))\n",
        "    \n",
        "    missing_years = expected_years - years_found\n",
        "    return missing_years\n",
        "\n",
        "\n",
        "async def connect_and_scrape(ticker_list, min_year: int = MIN_YEAR, check_missing: bool = True):\n",
        "    \"\"\"\n",
        "    è¿æ¥åˆ° Chrome å¹¶æŠ“å–æ‰€æœ‰ ticker çš„ URL\n",
        "    \n",
        "    Args:\n",
        "        ticker_list: è¦å¤„ç†çš„ ticker åˆ—è¡¨\n",
        "        min_year: æœ€å°å¹´ä»½ï¼ˆåªè·å–æ­¤å¹´ä»½åŠä»¥åçš„æ•°æ®ï¼‰\n",
        "        check_missing: æ˜¯å¦æ£€æŸ¥ç¼ºå¤±å¹´ä»½å¹¶é‡æ–°æŠ“å–\n",
        "    \"\"\"\n",
        "    print(\"ğŸš€ æ­£åœ¨è¿æ¥ Chromeï¼ˆç«¯å£ 9222ï¼‰...\")\n",
        "\n",
        "    async with async_playwright() as p:\n",
        "        # è¿æ¥åˆ°ä½ å·²ç»ç”¨ 9222 å¼€å¯çš„ Chrome\n",
        "        try:\n",
        "            browser = await p.chromium.connect_over_cdp(\"http://localhost:9222\")\n",
        "            context = browser.contexts[0] if browser.contexts else await browser.new_context()\n",
        "            print(\"âœ… æˆåŠŸè¿æ¥åˆ° Chrome\")\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ è¿æ¥å¤±è´¥: {e}\")\n",
        "            print(\"è¯·ç¡®ä¿å·²è¿è¡Œ: nohup /Applications/Google\\\\ Chrome.app/Contents/MacOS/Google\\\\ Chrome --remote-debugging-port=9222 --user-data-dir=/tmp/chrome_sa >/tmp/chrome_sa.log 2>&1 &\")\n",
        "            return\n",
        "\n",
        "        # è¿›åº¦æ–‡ä»¶è¯»å–\n",
        "        completed_tickers = get_completed_tickers()  # é»˜è®¤ progress.txtï¼ˆçœ‹ä½ å‡½æ•°é»˜è®¤å€¼ï¼‰\n",
        "        excluded_tickers = get_completed_tickers(progress_file=\"exclude_transcript.txt\")\n",
        "        failed_tickers = get_completed_tickers(progress_file=\"failed.txt\")\n",
        "\n",
        "        all_processed = completed_tickers.union(excluded_tickers).union(failed_tickers)\n",
        "        tickers_to_process = [t for t in ticker_list if t not in all_processed]\n",
        "\n",
        "        if not tickers_to_process:\n",
        "            print(\"ğŸ‰ æ‰€æœ‰ tickers å·²å®Œæˆ/å·²æ’é™¤/å·²å¤±è´¥ï¼Œæ— éœ€å†æ¬¡è¿è¡Œã€‚\")\n",
        "            await browser.close()\n",
        "            return\n",
        "\n",
        "        print(f\"ğŸ“„ å°†ä» {len(tickers_to_process)} ä¸ªæœªå®Œæˆçš„ tickers å¼€å§‹çˆ¬å–...\")\n",
        "\n",
        "        # è¾“å‡ºç›®å½•\n",
        "        INDEX_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        # ç¡®ä¿è®°å½•æ–‡ä»¶å­˜åœ¨\n",
        "        for fp in [\"exclude_transcript.txt\", \"failed.txt\"]:\n",
        "            if not os.path.exists(fp):\n",
        "                with open(fp, \"w\", encoding=\"utf-8\") as _:\n",
        "                    pass\n",
        "\n",
        "        # é€ä¸ª ticker å¤„ç†\n",
        "        for ticker in tickers_to_process:\n",
        "            base_url = f\"https://seekingalpha.com/symbol/{ticker}/earnings/transcripts\"\n",
        "            \n",
        "            # æ£€æŸ¥æ˜¯å¦å·²æœ‰ JSON æ–‡ä»¶\n",
        "            json_path = INDEX_DIR / f\"{ticker}.json\"\n",
        "            existing_results = []\n",
        "            if json_path.exists() and json_path.stat().st_size > 0:\n",
        "                try:\n",
        "                    with open(json_path, 'r', encoding='utf-8') as f:\n",
        "                        existing_results = json.load(f)\n",
        "                    print(f\"ğŸ“‹ {ticker}: å·²å­˜åœ¨ {len(existing_results)} æ¡ URL\")\n",
        "                    \n",
        "                    # æ£€æŸ¥ç¼ºå¤±å¹´ä»½\n",
        "                    if check_missing:\n",
        "                        missing_years = check_missing_years(existing_results, min_year)\n",
        "                        if missing_years:\n",
        "                            print(f\"âš ï¸ {ticker}: ç¼ºå¤±å¹´ä»½ {sorted(missing_years)}ï¼Œå°†é‡æ–°æŠ“å–\")\n",
        "                        else:\n",
        "                            print(f\"âœ… {ticker}: æ‰€æœ‰å¹´ä»½ï¼ˆ{min_year}å¹´åŠä»¥åï¼‰éƒ½å·²å­˜åœ¨ï¼Œè·³è¿‡\")\n",
        "                            mark_as_completed(ticker)\n",
        "                            continue\n",
        "                except json.JSONDecodeError:\n",
        "                    print(f\"âš ï¸ {ticker}: JSON æ–‡ä»¶æŸåï¼Œå°†é‡æ–°æŠ“å–\")\n",
        "                    existing_results = []\n",
        "\n",
        "            try:\n",
        "                # å¤ç”¨å·²æœ‰ seekingalpha æ ‡ç­¾é¡µï¼Œå¦åˆ™æ–°å¼€\n",
        "                page = None\n",
        "                for existing_page in context.pages:\n",
        "                    if \"seekingalpha.com\" in (existing_page.url or \"\"):\n",
        "                        page = existing_page\n",
        "                        break\n",
        "                if page is None:\n",
        "                    page = await context.new_page()\n",
        "\n",
        "                print(f\"\\nâ¡ï¸ æ­£åœ¨å¤„ç† {ticker}...\")\n",
        "                print(f\"â¡ï¸ å¯¼èˆªåˆ° {base_url}\")\n",
        "                await page.goto(base_url, timeout=60000)\n",
        "                await wait_for_captcha_and_resume(page)\n",
        "\n",
        "                # æ£€æŸ¥é¡µé¢æ˜¯å¦æœ‰ transcriptsï¼ˆæœ‰æ—¶è¯¥ ticker æ²¡å†…å®¹ï¼‰\n",
        "                print(\"ğŸ” æ£€æŸ¥é¡µé¢æ˜¯å¦åŒ…å« transcripts åˆ—è¡¨...\")\n",
        "                transcripts_button = await page.query_selector('a[data-test-id=\"earnings/transcripts\"]')\n",
        "                if not transcripts_button:\n",
        "                    print(f\"âš ï¸ {ticker} é¡µé¢ä¸åŒ…å« transcripts åŒºåŸŸï¼Œå†™ç©º JSON å¹¶æ ‡è®°æ’é™¤ã€‚\")\n",
        "\n",
        "                    file_path = os.path.join(INDEX_DIR, \"BM.json\")\n",
        "                    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
        "                        json.dump([], f, indent=2, ensure_ascii=False)\n",
        "\n",
        "                    mark_as_excluded(ticker)\n",
        "                    print(f\"âœ… å·²ä¿å­˜ç©ºæ–‡ä»¶: {file_path}\")\n",
        "                    continue\n",
        "\n",
        "                # âœ… åˆ†é¡µæŠ“å–ï¼šåªä¿ç•™ Earnings Call Transcript\n",
        "                print(\"ğŸ“„ ä½¿ç”¨åˆ†é¡µæ¨¡å¼æŠ“å–æ‰€æœ‰é¡µé¢ï¼ˆåªä¿ç•™ Earnings Call Transcriptï¼‰...\")\n",
        "\n",
        "                results = []\n",
        "                seen_links = set()\n",
        "                page_num = 1\n",
        "\n",
        "                while True:\n",
        "                    # é™åˆ¶ï¼šæœ€å¤šæŠ“å–10é¡µï¼Œé¿å…æ— é™å¾ªç¯\n",
        "                    if page_num > 10:\n",
        "                        print(f\"ğŸ›‘ å·²è¾¾åˆ°æœ€å¤§é¡µæ•°é™åˆ¶ï¼ˆ10é¡µï¼‰ï¼Œåœæ­¢ç¿»é¡µã€‚\")\n",
        "                        break\n",
        "                    paged_url = f\"{base_url}?page={page_num}\"\n",
        "                    print(f\"â¡ï¸ æŠ“å–ç¬¬ {page_num} é¡µ: {paged_url}\")\n",
        "\n",
        "                    await page.goto(paged_url, timeout=60000)\n",
        "                    await wait_for_captcha_and_resume(page)\n",
        "\n",
        "                    # å¦‚æœè¿™ä¸€é¡µè¿åˆ—è¡¨éƒ½ç­‰ä¸åˆ°ï¼Œè¯´æ˜æ²¡å†…å®¹/åˆ°å¤´äº†\n",
        "                    try:\n",
        "                        await page.wait_for_selector(\n",
        "                            'h3 a[data-test-id=\"post-list-item-title\"]',\n",
        "                            timeout=15000\n",
        "                        )\n",
        "                    except Exception:\n",
        "                        print(f\"ğŸ›‘ ç¬¬ {page_num} é¡µæ²¡æœ‰æ£€æµ‹åˆ°æ–‡ç« åˆ—è¡¨ï¼Œåœæ­¢ç¿»é¡µã€‚\")\n",
        "                        break\n",
        "\n",
        "                    article_links = await page.query_selector_all(\n",
        "                        'h3 a[data-test-id=\"post-list-item-title\"]'\n",
        "                    )\n",
        "                    if not article_links:\n",
        "                        print(f\"ğŸ›‘ ç¬¬ {page_num} é¡µé“¾æ¥æ•°ä¸º 0ï¼Œåœæ­¢ç¿»é¡µã€‚\")\n",
        "                        break\n",
        "                    # âœ… æ–°å¢ï¼šæ£€æŸ¥â€œæ— å†…å®¹æç¤ºâ€\n",
        "                    no_content = await page.query_selector(\n",
        "                        \"text=There are currently no headlines available\"\n",
        "                    )\n",
        "                    if no_content:\n",
        "                        print(f\"ğŸ›‘ ç¬¬ {page_num} é¡µæ˜¾ç¤ºæ— å†…å®¹æç¤ºï¼Œåœæ­¢ç¿»é¡µã€‚\")\n",
        "                        break\n",
        "\n",
        "                    if not article_links:\n",
        "                        print(f\"ğŸ›‘ ç¬¬ {page_num} é¡µ article_links ä¸ºç©ºï¼Œåœæ­¢ç¿»é¡µã€‚\")\n",
        "                        break\n",
        "\n",
        "                    print(f\"ğŸ“„ ç¬¬ {page_num} é¡µæ‰¾åˆ° {len(article_links)} ä¸ªæ ‡é¢˜é“¾æ¥ï¼ˆæœªè¿‡æ»¤ï¼‰\")\n",
        "\n",
        "                    page_added = 0\n",
        "                    for link_elem in article_links:\n",
        "                        title = (await link_elem.inner_text() or \"\").strip()\n",
        "                        href = await link_elem.get_attribute(\"href\")\n",
        "                        if not href:\n",
        "                            continue\n",
        "\n",
        "                        full_link = f\"https://seekingalpha.com{href.split('#')[0]}\"\n",
        "\n",
        "                        # âœ… è¿‡æ»¤ï¼šåªä¿ç•™å­£åº¦ Earnings Call Transcript\n",
        "                        if \"Earnings Call Transcript\" not in title:\n",
        "                            continue\n",
        "\n",
        "                        # âœ… è¿‡æ»¤ï¼šåªä¿ç•™å½“å‰ ticker çš„ç”µè¯ï¼ˆé¿å… MS é¡µé¢æ··å…¥ EV ç­‰ï¼‰\n",
        "                        # è§„åˆ™ï¼šæ ‡é¢˜é‡Œå¿…é¡»åŒ…å« \"(TICKER)\"ï¼Œä¾‹å¦‚ \"Morgan Stanley (MS) ...\"\n",
        "                        ticker_tag = f\"({ticker})\"\n",
        "                        if ticker_tag not in title:\n",
        "                            continue\n",
        "                        \n",
        "                        # âœ… è¿‡æ»¤ï¼šåªä¿ç•™æŒ‡å®šå¹´ä»½åŠä»¥åçš„æ•°æ®\n",
        "                        year = extract_year_from_title(title)\n",
        "                        if year is None or year < min_year:\n",
        "                            continue\n",
        "\n",
        "                        # å»é‡ï¼ˆæ£€æŸ¥é“¾æ¥å’Œæ ‡é¢˜ï¼‰\n",
        "                        if full_link in seen_links:\n",
        "                            continue\n",
        "                        seen_links.add(full_link)\n",
        "                        \n",
        "                        # å¦‚æœå·²æœ‰ç»“æœï¼Œæ£€æŸ¥æ˜¯å¦é‡å¤\n",
        "                        if any(item.get('link') == full_link for item in existing_results):\n",
        "                            continue\n",
        "\n",
        "                        results.append({\"title\": title, \"link\": full_link})\n",
        "                        page_added += 1\n",
        "\n",
        "                    print(f\"âœ… ç¬¬ {page_num} é¡µè¿‡æ»¤åæ–°å¢ {page_added} æ¡ Earnings Call Transcript\")\n",
        "\n",
        "                    # å¯é€‰ï¼šå¦‚æœä»ç¬¬2é¡µå¼€å§‹è¿ç»­æ²¡æœ‰æ–°å¢ï¼Œé€šå¸¸åé¢ä¹Ÿä¸ä¼šæœ‰äº†\n",
        "                    # if page_added == 0 and page_num >= 2:\n",
        "                    #     print(\"ğŸ›‘ è¿ç»­é¡µæœªå‘ç° Earnings Call Transcriptï¼Œæå‰åœæ­¢ç¿»é¡µã€‚\")\n",
        "                    #     break\n",
        "\n",
        "                    page_num += 1\n",
        "                    await asyncio.sleep(random.uniform(1.0, 2.0))\n",
        "\n",
        "                # åˆå¹¶æ–°æ—§ç»“æœ\n",
        "                if existing_results:\n",
        "                    # åˆå¹¶å¹¶å»é‡\n",
        "                    existing_links = {item.get('link') for item in existing_results}\n",
        "                    new_results = [r for r in results if r.get('link') not in existing_links]\n",
        "                    all_results = existing_results + new_results\n",
        "                    print(f\"ğŸ“„ {ticker} æœ¬æ¬¡æ–°å¢ {len(new_results)} æ¡ï¼Œæ€»è®¡ {len(all_results)} æ¡\")\n",
        "                else:\n",
        "                    all_results = results\n",
        "                    print(f\"ğŸ“„ {ticker} æœ€ç»ˆå…±æ”¶é›†åˆ° {len(results)} æ¡ Earnings Call Transcriptï¼ˆå»é‡åï¼‰\")\n",
        "                \n",
        "                # å†æ¬¡è¿‡æ»¤ï¼šåªä¿ç•™å½“å‰ ticker çš„è®°å½•ï¼Œæ¸…ç†æ‰å†å² JSON ä¸­æ··å…¥çš„å…¶ä»–å…¬å¸ï¼ˆä¾‹å¦‚ EVï¼‰\n",
        "                ticker_tag = f\"({ticker})\"\n",
        "                all_results = [item for item in all_results if ticker_tag in item.get('title', '')]\n",
        "\n",
        "                # æŒ‰å¹´ä»½ç»Ÿè®¡\n",
        "                years_found = {}\n",
        "                for item in all_results:\n",
        "                    year = extract_year_from_title(item.get('title', ''))\n",
        "                    if year:\n",
        "                        years_found[year] = years_found.get(year, 0) + 1\n",
        "                print(f\"ğŸ“Š {ticker} å¹´ä»½åˆ†å¸ƒ: {dict(sorted(years_found.items()))}\")\n",
        "\n",
        "                # ä¿å­˜ç»“æœ\n",
        "                json_path = INDEX_DIR / f\"{ticker}.json\"\n",
        "                with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
        "                    json.dump(all_results, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "                mark_as_completed(ticker)\n",
        "                print(f\"âœ… {ticker} æ•°æ®å·²æˆåŠŸä¿å­˜è‡³ {json_path}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"âŒ æŠ“å– {ticker} å¤±è´¥: {e}\")\n",
        "                mark_as_failed(ticker)\n",
        "\n",
        "        await browser.close()\n",
        "        print(\"âœ… æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼Œæµè§ˆå™¨å·²å…³é—­ã€‚\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "195a2844",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "å¼€å§‹è·å–æ‰€æœ‰ ticker çš„ Earnings Call Transcript URL\n",
            "============================================================\n",
            "ç›®æ ‡å…¬å¸: ['JPM', 'BK', 'DAL', 'C', 'WFC', 'BAC', 'TSM', 'MS', 'GS', 'BLK', 'MTB', 'STT', 'PNC', 'BR']\n",
            "æ—¶é—´èŒƒå›´: 2023å¹´åŠä»¥å\n",
            "Chrome ç«¯å£: 9222\n",
            "\n",
            "è¯·ç¡®ä¿å·²è¿è¡Œä»¥ä¸‹å‘½ä»¤å¯åŠ¨ Chrome:\n",
            "nohup /Applications/Google\\ Chrome.app/Contents/MacOS/Google\\ Chrome \\\n",
            "  --remote-debugging-port=9222 \\\n",
            "  --user-data-dir=/tmp/chrome_sa \\\n",
            "  >/tmp/chrome_sa.log 2>&1 &\n",
            "============================================================\n",
            "ğŸš€ æ­£åœ¨è¿æ¥ Chromeï¼ˆç«¯å£ 9222ï¼‰...\n",
            "âœ… æˆåŠŸè¿æ¥åˆ° Chrome\n",
            "ğŸ“„ å°†ä» 1 ä¸ªæœªå®Œæˆçš„ tickers å¼€å§‹çˆ¬å–...\n",
            "\n",
            "â¡ï¸ æ­£åœ¨å¤„ç† CVX...\n",
            "â¡ï¸ å¯¼èˆªåˆ° https://seekingalpha.com/symbol/CVX/earnings/transcripts\n",
            "æ­£åœ¨æ£€æŸ¥æ˜¯å¦å­˜åœ¨éªŒè¯ç ...\n",
            "æœªæ£€æµ‹åˆ°éªŒè¯ç ï¼Œç»§ç»­æ‰§è¡Œã€‚\n",
            "ğŸ” æ£€æŸ¥é¡µé¢æ˜¯å¦åŒ…å« transcripts åˆ—è¡¨...\n",
            "ğŸ“„ ä½¿ç”¨åˆ†é¡µæ¨¡å¼æŠ“å–æ‰€æœ‰é¡µé¢ï¼ˆåªä¿ç•™ Earnings Call Transcriptï¼‰...\n",
            "â¡ï¸ æŠ“å–ç¬¬ 1 é¡µ: https://seekingalpha.com/symbol/CVX/earnings/transcripts?page=1\n",
            "æ­£åœ¨æ£€æŸ¥æ˜¯å¦å­˜åœ¨éªŒè¯ç ...\n",
            "\n",
            "==================================================\n",
            "æ£€æµ‹åˆ°éªŒè¯ç ï¼è¯·åœ¨æµè§ˆå™¨ä¸­æ‰‹åŠ¨å®ŒæˆéªŒè¯ã€‚\n",
            "å®Œæˆåï¼Œè¯·å›åˆ°æ­¤ç»ˆç«¯å¹¶æŒ‰ Enter é”®ç»§ç»­ã€‚\n",
            "==================================================\n",
            "\n",
            "éªŒè¯ç å·²è§£å†³ï¼Œç¨‹åºç»§ç»­ã€‚\n",
            "ğŸ“„ ç¬¬ 1 é¡µæ‰¾åˆ° 22 ä¸ªæ ‡é¢˜é“¾æ¥ï¼ˆæœªè¿‡æ»¤ï¼‰\n",
            "âœ… ç¬¬ 1 é¡µè¿‡æ»¤åæ–°å¢ 8 æ¡ Earnings Call Transcript\n",
            "â¡ï¸ æŠ“å–ç¬¬ 2 é¡µ: https://seekingalpha.com/symbol/CVX/earnings/transcripts?page=2\n",
            "æ­£åœ¨æ£€æŸ¥æ˜¯å¦å­˜åœ¨éªŒè¯ç ...\n",
            "æœªæ£€æµ‹åˆ°éªŒè¯ç ï¼Œç»§ç»­æ‰§è¡Œã€‚\n",
            "ğŸ“„ ç¬¬ 2 é¡µæ‰¾åˆ° 22 ä¸ªæ ‡é¢˜é“¾æ¥ï¼ˆæœªè¿‡æ»¤ï¼‰\n",
            "âœ… ç¬¬ 2 é¡µè¿‡æ»¤åæ–°å¢ 2 æ¡ Earnings Call Transcript\n",
            "â¡ï¸ æŠ“å–ç¬¬ 3 é¡µ: https://seekingalpha.com/symbol/CVX/earnings/transcripts?page=3\n",
            "æ­£åœ¨æ£€æŸ¥æ˜¯å¦å­˜åœ¨éªŒè¯ç ...\n",
            "æœªæ£€æµ‹åˆ°éªŒè¯ç ï¼Œç»§ç»­æ‰§è¡Œã€‚\n",
            "ğŸ“„ ç¬¬ 3 é¡µæ‰¾åˆ° 22 ä¸ªæ ‡é¢˜é“¾æ¥ï¼ˆæœªè¿‡æ»¤ï¼‰\n",
            "âœ… ç¬¬ 3 é¡µè¿‡æ»¤åæ–°å¢ 0 æ¡ Earnings Call Transcript\n",
            "â¡ï¸ æŠ“å–ç¬¬ 4 é¡µ: https://seekingalpha.com/symbol/CVX/earnings/transcripts?page=4\n",
            "æ­£åœ¨æ£€æŸ¥æ˜¯å¦å­˜åœ¨éªŒè¯ç ...\n",
            "æœªæ£€æµ‹åˆ°éªŒè¯ç ï¼Œç»§ç»­æ‰§è¡Œã€‚\n",
            "ğŸ“„ ç¬¬ 4 é¡µæ‰¾åˆ° 22 ä¸ªæ ‡é¢˜é“¾æ¥ï¼ˆæœªè¿‡æ»¤ï¼‰\n",
            "âœ… ç¬¬ 4 é¡µè¿‡æ»¤åæ–°å¢ 0 æ¡ Earnings Call Transcript\n",
            "â¡ï¸ æŠ“å–ç¬¬ 5 é¡µ: https://seekingalpha.com/symbol/CVX/earnings/transcripts?page=5\n",
            "æ­£åœ¨æ£€æŸ¥æ˜¯å¦å­˜åœ¨éªŒè¯ç ...\n",
            "æœªæ£€æµ‹åˆ°éªŒè¯ç ï¼Œç»§ç»­æ‰§è¡Œã€‚\n",
            "ğŸ“„ ç¬¬ 5 é¡µæ‰¾åˆ° 22 ä¸ªæ ‡é¢˜é“¾æ¥ï¼ˆæœªè¿‡æ»¤ï¼‰\n",
            "âœ… ç¬¬ 5 é¡µè¿‡æ»¤åæ–°å¢ 0 æ¡ Earnings Call Transcript\n",
            "â¡ï¸ æŠ“å–ç¬¬ 6 é¡µ: https://seekingalpha.com/symbol/CVX/earnings/transcripts?page=6\n",
            "æ­£åœ¨æ£€æŸ¥æ˜¯å¦å­˜åœ¨éªŒè¯ç ...\n",
            "æœªæ£€æµ‹åˆ°éªŒè¯ç ï¼Œç»§ç»­æ‰§è¡Œã€‚\n",
            "ğŸ“„ ç¬¬ 6 é¡µæ‰¾åˆ° 22 ä¸ªæ ‡é¢˜é“¾æ¥ï¼ˆæœªè¿‡æ»¤ï¼‰\n",
            "âœ… ç¬¬ 6 é¡µè¿‡æ»¤åæ–°å¢ 0 æ¡ Earnings Call Transcript\n",
            "â¡ï¸ æŠ“å–ç¬¬ 7 é¡µ: https://seekingalpha.com/symbol/CVX/earnings/transcripts?page=7\n",
            "æ­£åœ¨æ£€æŸ¥æ˜¯å¦å­˜åœ¨éªŒè¯ç ...\n",
            "æœªæ£€æµ‹åˆ°éªŒè¯ç ï¼Œç»§ç»­æ‰§è¡Œã€‚\n",
            "ğŸ“„ ç¬¬ 7 é¡µæ‰¾åˆ° 22 ä¸ªæ ‡é¢˜é“¾æ¥ï¼ˆæœªè¿‡æ»¤ï¼‰\n",
            "âœ… ç¬¬ 7 é¡µè¿‡æ»¤åæ–°å¢ 0 æ¡ Earnings Call Transcript\n",
            "â¡ï¸ æŠ“å–ç¬¬ 8 é¡µ: https://seekingalpha.com/symbol/CVX/earnings/transcripts?page=8\n",
            "æ­£åœ¨æ£€æŸ¥æ˜¯å¦å­˜åœ¨éªŒè¯ç ...\n",
            "æœªæ£€æµ‹åˆ°éªŒè¯ç ï¼Œç»§ç»­æ‰§è¡Œã€‚\n",
            "ğŸ“„ ç¬¬ 8 é¡µæ‰¾åˆ° 22 ä¸ªæ ‡é¢˜é“¾æ¥ï¼ˆæœªè¿‡æ»¤ï¼‰\n",
            "âœ… ç¬¬ 8 é¡µè¿‡æ»¤åæ–°å¢ 0 æ¡ Earnings Call Transcript\n",
            "â¡ï¸ æŠ“å–ç¬¬ 9 é¡µ: https://seekingalpha.com/symbol/CVX/earnings/transcripts?page=9\n",
            "æ­£åœ¨æ£€æŸ¥æ˜¯å¦å­˜åœ¨éªŒè¯ç ...\n",
            "æœªæ£€æµ‹åˆ°éªŒè¯ç ï¼Œç»§ç»­æ‰§è¡Œã€‚\n",
            "ğŸ“„ ç¬¬ 9 é¡µæ‰¾åˆ° 22 ä¸ªæ ‡é¢˜é“¾æ¥ï¼ˆæœªè¿‡æ»¤ï¼‰\n",
            "âœ… ç¬¬ 9 é¡µè¿‡æ»¤åæ–°å¢ 0 æ¡ Earnings Call Transcript\n",
            "â¡ï¸ æŠ“å–ç¬¬ 10 é¡µ: https://seekingalpha.com/symbol/CVX/earnings/transcripts?page=10\n",
            "æ­£åœ¨æ£€æŸ¥æ˜¯å¦å­˜åœ¨éªŒè¯ç ...\n",
            "æœªæ£€æµ‹åˆ°éªŒè¯ç ï¼Œç»§ç»­æ‰§è¡Œã€‚\n",
            "ğŸ“„ ç¬¬ 10 é¡µæ‰¾åˆ° 22 ä¸ªæ ‡é¢˜é“¾æ¥ï¼ˆæœªè¿‡æ»¤ï¼‰\n",
            "âœ… ç¬¬ 10 é¡µè¿‡æ»¤åæ–°å¢ 0 æ¡ Earnings Call Transcript\n",
            "ğŸ›‘ å·²è¾¾åˆ°æœ€å¤§é¡µæ•°é™åˆ¶ï¼ˆ10é¡µï¼‰ï¼Œåœæ­¢ç¿»é¡µã€‚\n",
            "ğŸ“„ CVX æœ€ç»ˆå…±æ”¶é›†åˆ° 10 æ¡ Earnings Call Transcriptï¼ˆå»é‡åï¼‰\n",
            "ğŸ“Š CVX å¹´ä»½åˆ†å¸ƒ: {2023: 4, 2024: 3, 2025: 3}\n",
            "âœ… CVX æ•°æ®å·²æˆåŠŸä¿å­˜è‡³ /Users/xinyuewang/Desktop/1.27/data/transcript_index/CVX.json\n",
            "âœ… æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼Œæµè§ˆå™¨å·²å…³é—­ã€‚\n"
          ]
        }
      ],
      "source": [
        "# ========== æ‰§è¡ŒæŠ“å– ==========\n",
        "# è¿è¡Œæ­¤ cell æ¥è·å–æ‰€æœ‰ 16 å®¶å…¬å¸çš„ URL\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"å¼€å§‹è·å–æ‰€æœ‰ ticker çš„ Earnings Call Transcript URL\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"ç›®æ ‡å…¬å¸: {TICKER_LIST}\")\n",
        "print(f\"æ—¶é—´èŒƒå›´: {MIN_YEAR}å¹´åŠä»¥å\")\n",
        "print(f\"Chrome ç«¯å£: 9222\")\n",
        "print(\"\\nè¯·ç¡®ä¿å·²è¿è¡Œä»¥ä¸‹å‘½ä»¤å¯åŠ¨ Chrome:\")\n",
        "print(\"nohup /Applications/Google\\\\ Chrome.app/Contents/MacOS/Google\\\\ Chrome \\\\\")\n",
        "print(\"  --remote-debugging-port=9222 \\\\\")\n",
        "print(\"  --user-data-dir=/tmp/chrome_sa \\\\\")\n",
        "print(\"  >/tmp/chrome_sa.log 2>&1 &\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# æ‰§è¡ŒæŠ“å–\n",
        "await connect_and_scrape([\"CVX\"], min_year=MIN_YEAR, check_missing=True)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
